{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chentong/anaconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "no module 'xformers'. Processing without...\n",
      "no module 'xformers'. Processing without...\n",
      "no module 'xformers'. Processing without...\n",
      "no module 'xformers'. Processing without...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# # SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('/mnt/sdc/project/Brain_Network/MindEyeV2/src/generative_models')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "[2024-01-02 22:04:06,081] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Setting batch_size to 16\n",
      "[2024-01-02 22:04:08,798] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-02 22:04:08,799] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "# ## UNCOMMENT BELOW SECTION AND COMMENT OUT DEEPSPEED SECTION TO AVOID USING DEEPSPEED ###\n",
    "# use_deepspeed = False\n",
    "# accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "# global_batch_size = batch_size = 16\n",
    "\n",
    "### DEEPSPEED INITIALIZATION ###\n",
    "use_deepspeed = True\n",
    "import deepspeed\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "if num_devices <= 1 and utils.is_interactive():\n",
    "    global_batch_size = batch_size = 16\n",
    "    print(f\"Setting batch_size to {batch_size}\")\n",
    "    # can emulate a distributed environment for deepspeed to work in jupyter notebook\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(np.random.randint(10000)+9000)\n",
    "    os.environ[\"RANK\"] = \"0\"\n",
    "    os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "    os.environ[\"GLOBAL_BATCH_SIZE\"] = str(global_batch_size) # set this to your batch size!\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]    \n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices\n",
    "    if num_devices <= 1:\n",
    "        os.environ[\"RANK\"] = \"0\"\n",
    "        os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# alter the deepspeed config according to your global and local batch size\n",
    "if local_rank == 0:\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "    config['train_batch_size'] = int(os.environ[\"GLOBAL_BATCH_SIZE\"])\n",
    "    config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "    config['bf16'] = {'enabled': False}\n",
    "    config['fp16'] = {'enabled': True}\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'w') as file:\n",
    "        json.dump(config, file)\n",
    "else:\n",
    "    # give some time for the local_rank=0 gpu to prep new deepspeed config file\n",
    "    time.sleep(10)\n",
    "deepspeed_plugin = DeepSpeedPlugin(\"deepspeed_config_stage2_cpuoffload.json\")\n",
    "accelerator = Accelerator(split_batches=False, deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 14815\n",
      "device: cuda:0\n",
      "Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'zero_optimization': {'stage': 2, 'contiguous_gradients': True, 'stage3_gather_16bit_weights_on_model_save': True, 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_prefetch_bucket_size': 10000000.0, 'stage3_param_persistence_threshold': 100000.0, 'reduce_bucket_size': 10000000.0, 'sub_group_size': 1000000000.0, 'offload_optimizer': {'device': 'cpu', 'nvme_path': '/scratch', 'pin_memory': True}, 'offload_param': {'device': 'none', 'nvme_path': '/scratch', 'buffer_size': 4000000000.0, 'pin_memory': True}}, 'aio': {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'train_batch_size': 16, 'train_micro_batch_size_per_gpu': 16, 'wall_clock_breakdown': False, 'zero_allow_untested_optimizer': True}\n",
      "\n",
      "distributed = True num_devices = 4 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: testing\n",
      "--data_path=/mnt/sdc/project/Brain_Network/MindEyeV2_org/fsx/proj-fmri/shared/natural-scenes-dataset                     --model_name=testing                     --no-multi_subject --subj=1 --batch_size=16 --no-blurry_recon --no-depth_recon --no-clip_text --num_sessions=3                     --clip_scale=1. --blur_scale=100. --depth_scale=100. --hidden_dim=1024 --seq_past=1 --seq_future=0                    --no-use_prior --prior_scale=30                     --max_lr=3e-4 --mixup_pct=.50 --num_epochs=12 --ckpt_interval=1 --no-use_image_aug --no-ckpt_saving\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"testing\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/fsx/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} --no-blurry_recon --no-depth_recon --no-clip_text --num_sessions=3 \\\n",
    "                    --clip_scale=1. --blur_scale=100. --depth_scale=100. --hidden_dim=1024 --seq_past=1 --seq_future=0\\\n",
    "                    --no-use_prior --prior_scale=30 \\\n",
    "                    --max_lr=3e-4 --mixup_pct=.50 --num_epochs=12 --ckpt_interval=1 --no-use_image_aug --no-ckpt_saving\"# --wandb_log\" #--resume_from_ckpt \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 3\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output depth reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_text\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to contrastively learn with clip text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from depth recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=1,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1028,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_past\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_future\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.3),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 16 num_iterations_per_epoch = 35 num_samples_per_epoch = 562\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "# nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])-3 # 3 sessions are withheld for algonauts\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 3 sessions\n",
      "/mnt/sdc/project/Brain_Network/MindEyeV2_org/fsx/proj-fmri/shared/natural-scenes-dataset/wds/subj01/train/{0..2}.tar\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/mnt/sdc/project/Brain_Network/MindEyeV2_org/fsx/proj-fmri/shared/natural-scenes-dataset/wds/subj01/test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n",
      "currently using 2 seq_len (chose 1 past behav and 0 future behav)\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "    print(train_url)\n",
    "    \n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # Load hdf5 data for betas, but don't put everything into memory\n",
    "    f = h5py.File(f'{data_path}/betas_all_subj0{s}_fp32.hdf5', 'r')\n",
    "    # f = h5py.File(f'{data_path}/betas_subj0{subj}_thresholded_wholebrain.hdf5', 'r')\n",
    "    \n",
    "    betas = f['betas'][:]\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")\n",
    "seq_len = seq_past + 1 + seq_future\n",
    "print(f\"currently using {seq_len} seq_len (chose {seq_past} past behav and {seq_future} future behav)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 73k NSD images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed7f081a-3599-4ff7-b042-998c2291085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load COCO images and captions\n",
    "\n",
    "# f = h5py.File('/fsx/proj-fmri/shared/mindeyev2_dataset/trainval_coco_images_224_float16.hdf5', 'r')\n",
    "# coco_images = f['images']#[:]\n",
    "# print(\"coco_images\", coco_images.shape)\n",
    "\n",
    "# coco_ids = np.load(\"trainval_coco_ids.npy\")\n",
    "# print(\"coco_ids\", len(coco_ids))\n",
    "# captions_dict = dict(np.load(\"trainval_coco_captions_dict.npy\", allow_pickle=True).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a9c68c-c3c9-4080-bd99-067c4486dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check dataloaders are working\n",
    "\n",
    "# test_vox_indices = []\n",
    "# test_73k_images = []\n",
    "# for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "#     test_vox_indices = np.append(test_vox_indices, behav[:,0,5].cpu().numpy())\n",
    "#     test_73k_images = np.append(test_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# test_vox_indices = test_vox_indices.astype(np.int16)\n",
    "# print(test_i, (test_i+1) * num_test, len(test_vox_indices))\n",
    "# print(\"---\\n\")\n",
    "\n",
    "# train_vox_indices = []\n",
    "# train_73k_images = []\n",
    "# for train_i, (behav, past_behav, future_behav, old_behav) in enumerate(train_dl):\n",
    "#     train_vox_indices = np.append(train_vox_indices, behav[:,0,5].long().cpu().numpy())\n",
    "#     train_73k_images = np.append(train_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# train_vox_indices = train_vox_indices.astype(np.int16)\n",
    "# print(train_i, (train_i+1) * batch_size, len(train_vox_indices))\n",
    "\n",
    "# all_vox_indices = np.hstack((train_vox_indices, test_vox_indices))\n",
    "# all_images = np.hstack((train_73k_images, test_73k_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "if clip_text:\n",
    "    tokenizer = get_tokenizer('ViT-H-14')\n",
    "    hookT = Hook(clip_model.transformer.resblocks[-1].ln_2)\n",
    "    def get_clip_text_embeddings(text):\n",
    "        tokens = tokenizer(text, context_length=clip_model.context_length).to(device)\n",
    "        clip_model.encode_text(tokens)\n",
    "        return hookT.outputs.permute(1,0,2)\n",
    "    clip_text_seq_dim = 77\n",
    "    clip_text_emb_dim = 1024\n",
    "    annots = np.load(\"/fsx/proj-fmri/shared/mindeyev2_dataset/COCO_73k_annots_curated.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if blurry_recon:\n",
    "#     from diffusers import AutoencoderKL\n",
    "#     autoenc = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/fsx/proj-fmri/shared/cache\")\n",
    "#     # autoenc.load_state_dict(torch.load('../train_logs/sdxl_vae_normed/best.pth')[\"model_state_dict\"])\n",
    "#     autoenc.eval()\n",
    "#     autoenc.requires_grad_(False)\n",
    "#     autoenc.to(device)\n",
    "#     utils.count_params(autoenc)\n",
    "\n",
    "if blurry_recon:\n",
    "    # from diffusers import VQModel\n",
    "    from diffusers import VQDiffusionPipeline\n",
    "    autoenc = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\", torch_dtype=data_type, cache_dir=\"/fsx/proj-fmri/shared/cache\")\n",
    "\n",
    "    # autoenc = VQModel.from_pretrained(\"/fsx/proj-fmri/shared/cache/models--microsoft--vq-diffusion-ithq/snapshots/3f796fb49ee559370dc638dea1d8116af131d993/vqvae\", torch_dtype=data_type)\n",
    "    autoenc = autoenc.vqvae\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c8eee-9834-437d-bb60-b38faef50138",
   "metadata": {},
   "source": [
    "#### downsampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1ba8dd-64c2-4ac9-947e-725b7f2e3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if blurry_recon:\n",
    "#     if utils.is_interactive(): display(utils.torch_to_Image(images[[30]]))\n",
    "\n",
    "#     input_batch = images[[30]].to(device)\n",
    "#     print(input_batch.shape)\n",
    "\n",
    "#     downsampled_image = nn.functional.interpolate(input_batch, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "#     re_upsampled_image = nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest')\n",
    "#     re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "#     print(re_upsampled_enc.shape)\n",
    "    \n",
    "#     if utils.is_interactive(): display(utils.torch_to_Image((autoenc.decode(re_upsampled_enc/0.18215).sample / 2 + 0.5).clamp(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a3a8-2bef-4e81-9b82-e154d26a1e1d",
   "metadata": {},
   "source": [
    "#### MiDaS depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f35573e2-95bf-463d-8937-68ad4c2c3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if depth_recon:\n",
    "    from controlnet_aux.midas import MidasDetector\n",
    "    \n",
    "    midas_depth = MidasDetector.from_pretrained(\n",
    "      \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\", cache_dir=\"/fsx/proj-fmri/shared/cache\").to(device)\n",
    "    midas_depth.model.eval()\n",
    "    midas_depth.model.requires_grad_(False)\n",
    "    midas_depth.model.to(device)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba3f9207-b98e-45da-baa6-5cfcfb2ae958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if depth_recon:\n",
    "    if utils.is_interactive(): display(utils.torch_to_Image(images[[30]]))\n",
    "\n",
    "    input_batch = images[[30,31]].float().to(device)\n",
    "    print(input_batch.shape)\n",
    "    \n",
    "    midas_emb = midas_depth.model(input_batch).unsqueeze(1)\n",
    "    print(midas_emb.shape)\n",
    "\n",
    "    prediction = utils.resize(midas_emb, 32) #/30).clamp(0,1).half() # 30 is roughly prediction.max()\n",
    "    print(prediction.shape)\n",
    "    \n",
    "    prediction = (prediction / prediction.view(prediction.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(prediction)).half()\n",
    "    midas_emb_size = prediction.flatten(1).shape[1]\n",
    "    print(\"midas_emb\", prediction.shape, prediction.min(), prediction.max())\n",
    "    print(\"midas_emb_size\", midas_emb_size)\n",
    "    \n",
    "    if utils.is_interactive(): display(utils.torch_to_Image(utils.resize(prediction, 224))) \n",
    "\n",
    "    if blurry_recon:\n",
    "        prediction = utils.resize(midas_emb, 128).half().repeat(1,3,1,1)\n",
    "        prediction = (prediction / prediction.view(prediction.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(prediction)).half()\n",
    "        prediction_enc = autoenc.encode(2*prediction-1).latents * 0.18215\n",
    "        print(\"vae midas_emb\", prediction_enc.shape, prediction_enc.min(), prediction_enc.max())\n",
    "    \n",
    "        if utils.is_interactive(): display(utils.torch_to_Image((autoenc.decode(prediction_enc/0.18215).sample / 2 + 0.5).clamp(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "103,094,274 total\n",
      "103,094,274 trainable\n",
      "param counts:\n",
      "103,094,274 total\n",
      "103,094,274 trainable\n",
      "torch.Size([2, 1, 14278]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "        self.temp = nn.Parameter(torch.Tensor([5.3]))\n",
    "        self.bias = nn.Parameter(torch.Tensor([-2.]))\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3602c333-d029-465c-8fb4-c3ccffdba6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "456,074,128 total\n",
      "456,074,128 trainable\n",
      "param counts:\n",
      "559,168,402 total\n",
      "559,168,402 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([1]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, seq_len=2, h=4096, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768, text_clip_size=768, text_out_dim=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        self.text_clip_size = text_clip_size\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_ln1 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mlp(seq_len, seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_ln2 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mlp(h, h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True)\n",
    "        self.clip_proj = self.projector(clip_size, clip_size)\n",
    "        if clip_text:\n",
    "            self.clin2 = nn.Linear(h * seq_len, text_out_dim, bias=True)\n",
    "            self.clip_proj_text = self.projector(text_clip_size, text_clip_size)\n",
    "\n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Sequential(\n",
    "                nn.Linear(out_dim, 4096, bias=True),\n",
    "                nn.LayerNorm(4096),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4096, 4096))\n",
    "            self.bgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=128,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "\n",
    "        if depth_recon:\n",
    "            self.dlin1 = nn.Sequential(\n",
    "                    nn.Linear(h, midas_emb_size),\n",
    "                    nn.Sigmoid(),\n",
    "                )\n",
    "            self.dgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.dupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=1,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128, 256],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            \n",
    "    def projector(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def ln(self, dim):\n",
    "        return nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors for blur and depth outputs\n",
    "        t,b,d = torch.Tensor([0.]), torch.Tensor([0.]), torch.Tensor([0.])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x.permute(0,2,1)\n",
    "        residual2 = x\n",
    "        for ln1, block1, ln2, block2 in zip(self.mixer_ln1, self.mixer_blocks1, self.mixer_ln2, self.mixer_blocks2):\n",
    "            # Layer norm before transpose\n",
    "            x = ln1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Channel mixing\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Embedding mixing\n",
    "            x = ln2(x)\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.clin1(x).reshape(len(x), -1, self.clip_size)\n",
    "        \n",
    "        c = self.clip_proj(backbone)\n",
    "        \n",
    "        if clip_text:\n",
    "            t = self.clin2(x)\n",
    "            t = self.clip_proj_text(t.reshape(len(t), -1, self.text_clip_size))\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = b.reshape(len(b), 256, 4, 4)\n",
    "            b = self.bgroupnorm(b)\n",
    "            b = self.bupsampler(b)\n",
    "            \n",
    "        if depth_recon:\n",
    "            d = self.dlin1(x) #.reshape(len(x), 1, 32, 32)\n",
    "            d = d.reshape(len(d), 256, 4, 4)\n",
    "            d = self.dgroupnorm(d)\n",
    "            d = self.dupsampler(d)\n",
    "        \n",
    "        return backbone, c, t, b, d\n",
    "\n",
    "if clip_text:\n",
    "    model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                              clip_size=clip_emb_dim, text_clip_size=clip_text_emb_dim,\n",
    "                              out_dim=clip_emb_dim*clip_seq_dim, text_out_dim=clip_text_emb_dim*clip_text_seq_dim) \n",
    "else:\n",
    "    model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, text_, blur_, depth_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, text_.shape, blur_.shape, depth_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "819,033,618 total\n",
      "819,033,602 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "        voxel2clip=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = '/fsx/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * image.shape[-1],\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 25704\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "819,033,618 total\n",
      "819,033,602 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "else:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    if use_deepspeed:\n",
    "        deepspeed.DeepSpeedEngine.save_checkpoint(model, save_dir=outdir, tag=tag)\n",
    "        ckpt_path = outdir+f'/{tag}/{tag}.npy'\n",
    "        np.save(ckpt_path, {\n",
    "            'epoch': epoch,\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs})\n",
    "    else:\n",
    "        ckpt_path = outdir+f'/{tag}.pth'\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "        del unwrapped_model\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    if use_deepspeed:\n",
    "        state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "        try:\n",
    "            model.module.load_state_dict(state_dict, strict=strict)\n",
    "        except:\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "        if load_epoch:\n",
    "            np_ckpt = np.load(outdir+f'/{tag}/{tag}.npy', allow_pickle=True).tolist()\n",
    "            globals()[\"epoch\"] = np_ckpt['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "    else:\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        if load_epoch:\n",
    "            globals()[\"epoch\"] = checkpoint['epoch']\n",
    "            print(\"Epoch\",epoch)\n",
    "        if load_optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if load_lr:\n",
    "            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        try:\n",
    "            model.module.load_state_dict(state_dict, strict=strict)\n",
    "        except:\n",
    "            model.load_state_dict(state_dict, strict=strict)\n",
    "        del checkpoint\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load saved ckpt model weights into current model\n",
    "if resume_from_ckpt:\n",
    "    load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /admin/home-paulscotti/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /admin/home-paulscotti/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.8520045280456543 seconds\n",
      "[2023-12-30 23:48:49,491] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-30 23:48:51,922] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-12-30 23:48:51,924] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-12-30 23:48:51,924] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2023-12-30 23:48:51,927] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2023-12-30 23:48:51,927] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2023-12-30 23:48:51,927] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2023-12-30 23:48:51,928] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 10000000\n",
      "[2023-12-30 23:48:51,928] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 500,000,000\n",
      "[2023-12-30 23:48:51,928] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True\n",
      "[2023-12-30 23:48:51,929] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False\n",
      "[2023-12-30 23:48:56,158] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states\n",
      "[2023-12-30 23:48:56,161] [INFO] [utils.py:803:see_memory_usage] MA 9.47 GB         Max_MA 9.47 GB         CA 9.55 GB         Max_CA 10 GB \n",
      "[2023-12-30 23:48:56,162] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 83.84 GB, percent = 7.5%\n",
      "[2023-12-30 23:48:59,383] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states\n",
      "[2023-12-30 23:48:59,384] [INFO] [utils.py:803:see_memory_usage] MA 9.47 GB         Max_MA 9.47 GB         CA 9.55 GB         Max_CA 10 GB \n",
      "[2023-12-30 23:48:59,385] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 93.34 GB, percent = 8.3%\n",
      "[2023-12-30 23:48:59,385] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized\n",
      "[2023-12-30 23:48:59,575] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-12-30 23:48:59,576] [INFO] [utils.py:803:see_memory_usage] MA 9.47 GB         Max_MA 9.47 GB         CA 9.55 GB         Max_CA 10 GB \n",
      "[2023-12-30 23:48:59,577] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 93.34 GB, percent = 8.3%\n",
      "[2023-12-30 23:48:59,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
      "[2023-12-30 23:48:59,585] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-12-30 23:48:59,586] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.OneCycleLR object at 0x7f40902a8690>\n",
      "[2023-12-30 23:48:59,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05], mom=[(0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999)]\n",
      "[2023-12-30 23:48:59,587] [INFO] [config.py:972:print] DeepSpeedEngine configuration:\n",
      "[2023-12-30 23:48:59,587] [INFO] [config.py:976:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-12-30 23:48:59,588] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-12-30 23:48:59,588] [INFO] [config.py:976:print]   amp_enabled .................. False\n",
      "[2023-12-30 23:48:59,588] [INFO] [config.py:976:print]   amp_params ................... False\n",
      "[2023-12-30 23:48:59,589] [INFO] [config.py:976:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-12-30 23:48:59,589] [INFO] [config.py:976:print]   bfloat16_enabled ............. False\n",
      "[2023-12-30 23:48:59,589] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-12-30 23:48:59,589] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-12-30 23:48:59,590] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-12-30 23:48:59,590] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f40903563d0>\n",
      "[2023-12-30 23:48:59,590] [INFO] [config.py:976:print]   communication_data_type ...... None\n",
      "[2023-12-30 23:48:59,591] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-12-30 23:48:59,591] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False\n",
      "[2023-12-30 23:48:59,591] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False\n",
      "[2023-12-30 23:48:59,591] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-12-30 23:48:59,592] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False\n",
      "[2023-12-30 23:48:59,592] [INFO] [config.py:976:print]   dataloader_drop_last ......... False\n",
      "[2023-12-30 23:48:59,592] [INFO] [config.py:976:print]   disable_allgather ............ False\n",
      "[2023-12-30 23:48:59,592] [INFO] [config.py:976:print]   dump_state ................... False\n",
      "[2023-12-30 23:48:59,593] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-12-30 23:48:59,593] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False\n",
      "[2023-12-30 23:48:59,593] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-12-30 23:48:59,593] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-12-30 23:48:59,594] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-12-30 23:48:59,595] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-12-30 23:48:59,595] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-12-30 23:48:59,596] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-12-30 23:48:59,596] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False\n",
      "[2023-12-30 23:48:59,596] [INFO] [config.py:976:print]   elasticity_enabled ........... False\n",
      "[2023-12-30 23:48:59,597] [INFO] [config.py:976:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-12-30 23:48:59,597] [INFO] [config.py:976:print]   fp16_auto_cast ............... False\n",
      "[2023-12-30 23:48:59,597] [INFO] [config.py:976:print]   fp16_enabled ................. True\n",
      "[2023-12-30 23:48:59,597] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-12-30 23:48:59,598] [INFO] [config.py:976:print]   global_rank .................. 0\n",
      "[2023-12-30 23:48:59,598] [INFO] [config.py:976:print]   grad_accum_dtype ............. None\n",
      "[2023-12-30 23:48:59,598] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 1\n",
      "[2023-12-30 23:48:59,598] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0\n",
      "[2023-12-30 23:48:59,599] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-12-30 23:48:59,599] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-12-30 23:48:59,599] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-12-30 23:48:59,599] [INFO] [config.py:976:print]   load_universal_checkpoint .... False\n",
      "[2023-12-30 23:48:59,600] [INFO] [config.py:976:print]   loss_scale ................... 0\n",
      "[2023-12-30 23:48:59,600] [INFO] [config.py:976:print]   memory_breakdown ............. False\n",
      "[2023-12-30 23:48:59,600] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False\n",
      "[2023-12-30 23:48:59,601] [INFO] [config.py:976:print]   mics_shard_size .............. -1\n",
      "[2023-12-30 23:48:59,602] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-12-30 23:48:59,602] [INFO] [config.py:976:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-12-30 23:48:59,602] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-12-30 23:48:59,602] [INFO] [config.py:976:print]   optimizer_name ............... None\n",
      "[2023-12-30 23:48:59,603] [INFO] [config.py:976:print]   optimizer_params ............. None\n",
      "[2023-12-30 23:48:59,603] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-12-30 23:48:59,603] [INFO] [config.py:976:print]   pld_enabled .................. False\n",
      "[2023-12-30 23:48:59,604] [INFO] [config.py:976:print]   pld_params ................... False\n",
      "[2023-12-30 23:48:59,604] [INFO] [config.py:976:print]   prescale_gradients ........... False\n",
      "[2023-12-30 23:48:59,604] [INFO] [config.py:976:print]   scheduler_name ............... None\n",
      "[2023-12-30 23:48:59,604] [INFO] [config.py:976:print]   scheduler_params ............. None\n",
      "[2023-12-30 23:48:59,604] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2023-12-30 23:48:59,605] [INFO] [config.py:976:print]   sparse_attention ............. None\n",
      "[2023-12-30 23:48:59,605] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False\n",
      "[2023-12-30 23:48:59,605] [INFO] [config.py:976:print]   steps_per_print .............. inf\n",
      "[2023-12-30 23:48:59,606] [INFO] [config.py:976:print]   train_batch_size ............. 16\n",
      "[2023-12-30 23:48:59,606] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  16\n",
      "[2023-12-30 23:48:59,606] [INFO] [config.py:976:print]   use_node_local_storage ....... False\n",
      "[2023-12-30 23:48:59,606] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False\n",
      "[2023-12-30 23:48:59,607] [INFO] [config.py:976:print]   weight_quantization_config ... None\n",
      "[2023-12-30 23:48:59,607] [INFO] [config.py:976:print]   world_size ................... 1\n",
      "[2023-12-30 23:48:59,607] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  True\n",
      "[2023-12-30 23:48:59,607] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=10000000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=PosixPath('/scratch'), buffer_count=5, buffer_size=4000000000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/scratch'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=10000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-12-30 23:48:59,608] [INFO] [config.py:976:print]   zero_enabled ................. True\n",
      "[2023-12-30 23:48:59,608] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-12-30 23:48:59,608] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2\n",
      "[2023-12-30 23:48:59,609] [INFO] [config.py:962:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_prefetch_bucket_size\": 1.000000e+07, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+05, \n",
      "        \"reduce_bucket_size\": 1.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"buffer_size\": 4.000000e+09, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 2.621440e+07, \n",
      "        \"queue_depth\": 32, \n",
      "        \"thread_count\": 1, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 16, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing starting with epoch 0 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2023-12-30 23:50:26,715] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-paulscotti/mindeye/lib/python3.11/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1881: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-30 23:50:27,299] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2023-12-30 23:50:27,879] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2023-12-30 23:50:28,461] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2023-12-30 23:50:29,041] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2023-12-30 23:50:29,623] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2023-12-30 23:50:30,205] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2023-12-30 23:50:30,786] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2023-12-30 23:50:31,365] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2023-12-30 23:50:31,945] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2023-12-30 23:50:32,524] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2023-12-30 23:50:33,105] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2023-12-30 23:50:33,688] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2023-12-30 23:50:34,270] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2023-12-30 23:50:34,850] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2023-12-30 23:50:35,433] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2023-12-30 23:50:36,013] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2023-12-30 23:50:36,594] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2023-12-30 23:50:37,175] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2023-12-30 23:50:37,756] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "[2023-12-31 00:18:13,318] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "skip_train = True if epoch>=(num_epochs-1) else False # skip training if you are resuming from a fully trained model\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "    \n",
    "    fwd_text_percent_correct = 0.\n",
    "    bwd_text_percent_correct = 0.\n",
    "    test_fwd_text_percent_correct = 0.\n",
    "    test_bwd_text_percent_correct = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_depth_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    test_loss_blurry_total = 0.\n",
    "    test_loss_depth_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    depth_pixcorr = 0.\n",
    "    test_depth_pixcorr = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):    \n",
    "                image0 = images[behav0[:,0,0].cpu().long()].float()\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = image0\n",
    "                \n",
    "                if clip_text:\n",
    "                    annot_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = utils.select_annotations(annots[behav0[:,0,0].cpu().long()])\n",
    "\n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                voxel0 = torch.Tensor(voxel0)\n",
    "                # ----- org ----- \n",
    "                # past_behavior = past_behav0[:,:(seq_len-1),5].cpu().long()\n",
    "                # ----- changed -----\n",
    "                # past\n",
    "                past_hebavior = past_behav0[:,:(seq_past),5].cpu().long()\n",
    "                past_voxel0 = voxels[f'subj0{subj_list[s]}'][past_behavior]\n",
    "                past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                past_voxel0 = torch.Tensor(past_voxel0)\n",
    "                # future\n",
    "                future_behavior = future_behav0[:,:(seq_future),5].cpu().long()\n",
    "                future_voxel0 = voxels[f'subj0{subj_list[s]}'][future_behavior]\n",
    "                future_voxel0[future_behavior==-1] = voxel0[torch.where(future_behavior==-1)[0]] # replace invalid past voxels \n",
    "                future_voxel0 = torch.Tensor(future_voxel0)\n",
    "                # if shared100, then you need to mask it out \n",
    "                for p in range(seq_past):\n",
    "                    if past_behav0[:,p,-1] == 1: \n",
    "                        past_voxel0[p] = torch.zeros_like(past_voxel0[p])\n",
    "                for p in range(seq_past):\n",
    "                    if future_behav0[:,p,-1] == 1: \n",
    "                        future_voxel0[p] = torch.zeros_like(future_voxel0[p])\n",
    "                # concat with current seq\n",
    "                voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                voxel0 = torch.cat((voxel0.unsqueeze(1), future_voxel0), axis=1)\n",
    "                # voxel0 = torch.hstack((voxel0, past_voxel0.flatten(1))).unsqueeze(1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    if skip_train is False:\n",
    "        for train_i in range(num_iterations_per_epoch):\n",
    "            with torch.cuda.amp.autocast(dtype=data_type):\n",
    "                optimizer.zero_grad()\n",
    "                loss=0.\n",
    "\n",
    "                voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                image = image_iters[train_i].detach().to(device)\n",
    "                if clip_text:\n",
    "                    annot = [annot_iters[f\"subj0{s}_iter{train_i}\"] for s in subj_list]\n",
    "\n",
    "                # if not epoch < int(mixup_pct * num_epochs):\n",
    "                #     extra_image = coco_images[np.random.choice(len(coco_images), batch_size, replace=False)].to(device).float()\n",
    "                #     image = torch.vstack((image, extra_image))\n",
    "\n",
    "                if blurry_recon:\n",
    "                    ran = np.random.rand()\n",
    "                    if ran > .66:\n",
    "                        blurry_image = utils.resize(transforms.GaussianBlur(kernel_size=(15,15),sigma=(12,12))(image), 128)\n",
    "                        # utils.resize(nn.functional.interpolate(image, size=(4, 4), mode='bilinear', align_corners=False),128)\n",
    "                    elif ran > .33:\n",
    "                        blurry_image = utils.resize(transforms.GaussianBlur(kernel_size=(115,115),sigma=(112,112))(image), 128)\n",
    "                        # utils.resize(nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False),128)\n",
    "                    else:\n",
    "                        blurry_image = utils.resize(nn.functional.interpolate(image, size=(12, 12), mode='bilinear', align_corners=False), 128)\n",
    "\n",
    "                    blurry_image_enc = autoenc.encode(2*blurry_image-1).latents * 0.18215\n",
    "\n",
    "                if depth_recon:\n",
    "                    # depth_images = utils.resize(midas_depth.model(image).unsqueeze(1).repeat(1,3,1,1), 128)\n",
    "                    depth_images = utils.resize(midas_depth.model(image).unsqueeze(1), 32) # batch x 1 x 32 x 32\n",
    "                    depth_images = (depth_images / depth_images.view(depth_images.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(depth_images)).half()\n",
    "                    # depth_images = nn.functional.interpolate(depth_images, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    depth_image = depth_images # autoenc.encode(2*depth_images-1).latents * 0.18215\n",
    "\n",
    "                if use_image_aug: \n",
    "                    image = img_augment(image)\n",
    "\n",
    "                clip_target = clip_img_embedder(image)\n",
    "                if clip_text: clip_text_target = get_clip_text_embeddings(annot[0])\n",
    "                assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    perm = torch.cat(perm_list, dim=0)\n",
    "                    betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    betas = torch.cat(betas_list, dim=0)\n",
    "                    select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    select = torch.cat(select_list, dim=0)\n",
    "\n",
    "                voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "                backbone, clip_voxels, clip_text_voxels, blurry_image_enc_, depth_image_ = model.backbone(voxel_ridge)\n",
    "                # backbone = utils.prep_for_prior(backbone)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if use_prior:\n",
    "                    # clip_target_prior = utils.prep_for_prior(clip_target)\n",
    "                    loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    recon_mse += mse(prior_out, clip_target).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    clip_text_voxels_norm = nn.functional.normalize(clip_text_voxels.flatten(1), dim=-1)\n",
    "                    clip_text_target_norm = nn.functional.normalize(clip_text_target.flatten(1), dim=-1)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    if epoch < int(mixup_pct * num_epochs):                \n",
    "                        loss_clip = utils.mixco_nce(\n",
    "                            clip_voxels_norm,\n",
    "                            clip_target_norm,\n",
    "                            temp=.006,\n",
    "                            perm=perm, betas=betas, select=select)\n",
    "                        if clip_text:\n",
    "                            loss_clip += utils.mixco_nce(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=.006,\n",
    "                                perm=perm, betas=betas, select=select)\n",
    "                    else:\n",
    "                        epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                        loss_clip = utils.soft_clip_loss(\n",
    "                            clip_voxels_norm,\n",
    "                            clip_target_norm,\n",
    "                            temp=epoch_temp)\n",
    "                        if clip_text:\n",
    "                            loss_clip += utils.soft_clip_loss(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=epoch_temp)\n",
    "\n",
    "                    loss_clip_total += loss_clip.item()\n",
    "                    loss_clip *= clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    # downsampled_image = nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    # re_upsampled_image = utils.add_saturation(nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest'))\n",
    "                    # re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc) #+ l1(blurry_image_enc_, re_upsampled_enc))\n",
    "                    # loss_blurry += l1(torch.var(blurry_image_enc), torch.var(blurry_image_enc_))\n",
    "                    # loss_blurry -= compute_negative_l1_losses(blurry_image_enc_.flatten(1), blurry_image_enc.flatten(1)) * 1e-5\n",
    "                    loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "\n",
    "                if depth_recon:\n",
    "                    loss_depth = l1(depth_image_, depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(depth_image))\n",
    "                    # quantized_depth_image = torch.round(depth_image * 5) / 5\n",
    "                    # loss_depth = l1(depth_image_, quantized_depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(quantized_depth_image))\n",
    "                    # loss_depth -= compute_negative_l1_losses(depth_image_.flatten(1), depth_image.flatten(1)) * 1e-5\n",
    "                    loss_depth_total += loss_depth.item()\n",
    "                    loss_depth *= depth_scale\n",
    "                    loss += loss_depth\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    fwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_voxels_norm, clip_text_target_norm), labels, k=1).item()\n",
    "                    bwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_target_norm, clip_text_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                if blurry_recon:\n",
    "                    with torch.no_grad():\n",
    "                        # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                        random_samps = np.random.choice(np.arange(len(image)), size=batch_size//5, replace=False)\n",
    "                        blurry_recon_images = (autoenc.decode(blurry_image_enc_[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                        pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                        blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if depth_recon:\n",
    "                    with torch.no_grad():\n",
    "                        pixcorr = utils.pixcorr(depth_image, depth_image_)\n",
    "                        depth_pixcorr += pixcorr.item()\n",
    "\n",
    "                utils.check_loss(loss)\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                if lr_scheduler_type is not None:\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "\n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                    \n",
    "                    #  ----- org -----\n",
    "                    # past_behavior = past_behav[:,:(seq_len-1),5].cpu().long()\n",
    "                    # ----- changed -----\n",
    "                    # past\n",
    "                    past_behavior = past_behav[:,:(seq_past),5].cpu().long()\n",
    "                    past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                    # future\n",
    "                    future_behavior = future_behav[:,:(seq_future),5].cpu().long()\n",
    "                    future_voxels = voxels[f'subj0{subj}'][future_behavior]                    \n",
    "                    if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                        past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "                    if torch.any(future_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                        past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                    voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                    voxel = torch.cat((voxel.unsqueeze(1), future_voxels), axis=1)\n",
    "                    # voxel = torch.hstack((voxel, past_voxels.flatten(1))).unsqueeze(1)\n",
    "\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            #test_voxel = torch.mean(voxel[locs],axis=0)[None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                            # if seq_len > 1:\n",
    "                            #     test_past_voxel = past_voxels[locs][None]\n",
    "                            if clip_text: test_annot = utils.select_annotations(annots[[im]])\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            # test_voxel = torch.vstack((test_voxel, torch.mean(voxel[locs],axis=0)[None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "                            # if seq_len > 1:\n",
    "                            #     test_past_voxel = torch.vstack((test_past_voxel, past_voxels[locs][None]))\n",
    "                            if clip_text: test_annot = np.vstack((test_annot,utils.select_annotations(annots[[im]])))\n",
    "\n",
    "                loss=0.\n",
    "                            \n",
    "                test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                # if seq_len > 1: \n",
    "                #     past_voxel = test_past_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
    "                if clip_text: annot = test_annot[test_indices]\n",
    "                assert len(image) == 300\n",
    "\n",
    "                if blurry_recon:\n",
    "                    blurry_image_enc = autoenc.encode(2*utils.resize(image,128)-1).latents * 0.18215\n",
    "\n",
    "                if depth_recon:\n",
    "                    depth_images = utils.resize(midas_depth.model(image).unsqueeze(1), 32)\n",
    "                    depth_images = (depth_images / depth_images.view(depth_images.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(depth_images)).half()\n",
    "                    # depth_images = nn.functional.interpolate(depth_images, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    depth_image = depth_images\n",
    "\n",
    "                clip_target = clip_img_embedder(image.float())\n",
    "                if clip_text: clip_text_target = get_clip_text_embeddings(annot.flatten())\n",
    "\n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep],0) # 0th index of subj_list\n",
    "                    # if seq_len > 1:\n",
    "                    #     past_voxel_ridge = model.ridge(past_voxel[:,rep],0)\n",
    "                    #     voxel_ridge = torch.cat((voxel_ridge, past_voxel_ridge), axis=1)\n",
    "                    backbone0, clip_voxels0, clip_text_voxels, blurry_image_enc_, depth_image_ = model.backbone(voxel_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
    "                # backbone = utils.prep_for_prior(backbone)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if use_prior:\n",
    "                    # clip_target_prior = utils.prep_for_prior(clip_target)\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                    prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                                    text_cond = dict(text_embed = backbone), \n",
    "                                    cond_scale = 1., timesteps = timesteps)\n",
    "                    \n",
    "                    test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    test_recon_mse += mse(prior_out, clip_target).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    clip_text_voxels_norm = nn.functional.normalize(clip_text_voxels.flatten(1), dim=-1)\n",
    "                    clip_text_target_norm = nn.functional.normalize(clip_text_target.flatten(1), dim=-1)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "                    if clip_text:\n",
    "                        loss_clip_text = utils.soft_clip_loss(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=.006)\n",
    "                        loss_clip += loss_clip_text\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    # downsampled_image = nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    # re_upsampled_image = utils.add_saturation(nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest'))\n",
    "                    # re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc) #+ l1(blurry_image_enc_, re_upsampled_enc))\n",
    "                    # loss_blurry += l1(torch.var(blurry_image_enc), torch.var(blurry_image_enc_))\n",
    "                    # loss_blurry -= compute_negative_l1_losses(blurry_image_enc_.flatten(1), blurry_image_enc.flatten(1)) * 1e-5\n",
    "                    test_loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "\n",
    "                    # halving the batch size because the decoder is computationally heavy\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[:len(image)//2]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    blurry_recon_images = torch.vstack((blurry_recon_images, (autoenc.decode(blurry_image_enc_[len(image)//2:]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    pixcorr = utils.pixcorr(image, blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if depth_recon:\n",
    "                    loss_depth = l1(depth_image_, depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(depth_image))\n",
    "                    # quantized_depth_image = torch.round(depth_image * 5) / 5\n",
    "                    # loss_depth = l1(depth_image_, quantized_depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(quantized_depth_image))\n",
    "                    # loss_depth -= compute_negative_l1_losses(depth_image_.flatten(1), depth_image.flatten(1)) * 1e-5\n",
    "                    test_loss_depth_total += loss_depth.item()\n",
    "                    loss_depth *= depth_scale\n",
    "                    loss += loss_depth\n",
    "\n",
    "                    pixcorr = utils.pixcorr(depth_image, depth_image_)\n",
    "                    test_depth_pixcorr += pixcorr.item()\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    test_fwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_voxels_norm, clip_text_target_norm), labels, k=1).item()\n",
    "                    test_bwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_target_norm, clip_text_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            if skip_train: break\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/fwd_text_pct_correct\": fwd_text_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_text_pct_correct\": bwd_text_percent_correct / (train_i + 1),\n",
    "                \"test/test_text_fwd_pct_correct\": test_fwd_text_percent_correct / (test_i + 1),\n",
    "                \"test/test_text_bwd_pct_correct\": test_bwd_text_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"test/loss_blurry_total\": test_loss_blurry_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/depth_pixcorr\": depth_pixcorr / (train_i + 1),\n",
    "                \"test/depth_pixcorr\": test_depth_pixcorr / (test_i + 1),\n",
    "                \"train/loss_depth_total\": loss_depth_total / (train_i + 1),\n",
    "                \"test/loss_depth_total\": test_loss_depth_total / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc_[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "\n",
    "                if depth_recon:\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    # axes[0].imshow(utils.torch_to_Image((autoenc.decode(depth_image[[0]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    # axes[1].imshow(utils.torch_to_Image((autoenc.decode(depth_image_[[0]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image(utils.resize(depth_image[[j]].view(1,1,32,32).clamp(0,1), 224)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image(utils.resize(depth_image_[[j]].view(1,1,32,32).clamp(0,1), 224)))\n",
    "                        axes[jj].axis('off')\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/depth_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "                        \n",
    "                if use_prior and visualize_prior: # output recons every ckpt\n",
    "                    idx = 1\n",
    "                    print(f\"reconstructing... idx={idx}\")\n",
    "                    # samples = utils.unclip_recon(prior_out,\n",
    "                    #                              diffusion_engine,\n",
    "                    #                              vector_suffix)\n",
    "                    samples = utils.unclip_recon(prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                    if wandb_log:\n",
    "                        if epoch==0:\n",
    "                            logs[f\"test/orig\"] = wandb.Image(transforms.ToPILImage()(image[idx]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                        logs[f\"test/recons\"] = wandb.Image(transforms.ToPILImage()(samples[0]),\n",
    "                                                           caption=f\"epoch{epoch:03d}\")\n",
    "                    if utils.is_interactive():\n",
    "                        if epoch==0:\n",
    "                            plt.figure(figsize=(2,2))\n",
    "                            plt.imshow(transforms.ToPILImage()(image[idx]))\n",
    "                            plt.axis('off')\n",
    "                            plt.show()\n",
    "                        \n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samples[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "                        \n",
    "                        samplesX = utils.unclip_recon(contaminated_prior_out[[idx]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix)\n",
    "                        plt.figure(figsize=(2,2))\n",
    "                        plt.imshow(transforms.ToPILImage()(samplesX[0]))\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
