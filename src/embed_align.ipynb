{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "from torch.utils.data import Dataset\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n",
      "Setting batch_size to 16\n",
      "[2024-01-04 06:21:37,653] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-01-04 06:21:37,654] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "\n",
    "# ## UNCOMMENT BELOW SECTION AND COMMENT OUT DEEPSPEED SECTION TO AVOID USING DEEPSPEED ###\n",
    "# use_deepspeed = False\n",
    "# accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\") # ['no', 'fp8', 'fp16', 'bf16']\n",
    "# global_batch_size = batch_size = 16\n",
    "\n",
    "# ### DEEPSPEED INITIALIZATION ###\n",
    "use_deepspeed = True\n",
    "import deepspeed\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "if num_devices <= 8 and utils.is_interactive():\n",
    "    global_batch_size = batch_size = 16\n",
    "    print(f\"Setting batch_size to {batch_size}\")\n",
    "    # can emulate a distributed environment for deepspeed to work in jupyter notebook\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = str(np.random.randint(10000)+9000)\n",
    "    os.environ[\"RANK\"] = \"0\"\n",
    "    os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "    os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "    os.environ[\"GLOBAL_BATCH_SIZE\"] = str(global_batch_size) # set this to your batch size!\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]    \n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices\n",
    "    if num_devices <= 1:\n",
    "        os.environ[\"RANK\"] = \"0\"\n",
    "        os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "        os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "\n",
    "# alter the deepspeed config according to your global and local batch size\n",
    "if local_rank == 0:\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'r') as file:\n",
    "        config = json.load(file)\n",
    "    config['train_batch_size'] = int(os.environ[\"GLOBAL_BATCH_SIZE\"])\n",
    "    config['train_micro_batch_size_per_gpu'] = batch_size\n",
    "    config['bf16'] = {'enabled': False}\n",
    "    config['fp16'] = {'enabled': True}\n",
    "    with open('deepspeed_config_stage2_cpuoffload.json', 'w') as file:\n",
    "        json.dump(config, file)\n",
    "else:\n",
    "    # give some time for the local_rank=0 gpu to prep new deepspeed config file\n",
    "    time.sleep(10)\n",
    "deepspeed_plugin = DeepSpeedPlugin(\"deepspeed_config_stage2_cpuoffload.json\")\n",
    "accelerator = Accelerator(split_batches=False, deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 4193308\n",
      "device: cuda:0\n",
      "Distributed environment: DEEPSPEED  Backend: nccl\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda:0\n",
      "\n",
      "Mixed precision type: fp16\n",
      "ds_config: {'bf16': {'enabled': False}, 'fp16': {'enabled': True}, 'zero_optimization': {'stage': 2, 'contiguous_gradients': True, 'stage3_gather_16bit_weights_on_model_save': True, 'stage3_max_live_parameters': 1000000000.0, 'stage3_max_reuse_distance': 1000000000.0, 'stage3_prefetch_bucket_size': 10000000.0, 'stage3_param_persistence_threshold': 100000.0, 'reduce_bucket_size': 10000000.0, 'sub_group_size': 1000000000.0, 'offload_optimizer': {'device': 'cpu', 'nvme_path': '/scratch', 'pin_memory': True}, 'offload_param': {'device': 'none', 'nvme_path': '/scratch', 'buffer_size': 4000000000.0, 'pin_memory': True}}, 'aio': {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}, 'gradient_accumulation_steps': 1, 'gradient_clipping': 1.0, 'steps_per_print': inf, 'train_batch_size': 16, 'train_micro_batch_size_per_gpu': 16, 'wall_clock_breakdown': False, 'zero_allow_untested_optimizer': True}\n",
      "\n",
      "distributed = True num_devices = 8 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "# set data_type to match your mixed precision (automatically set based on deepspeed config)\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: embedsOnly\n",
      "--data_path=/weka/proj-fmri/shared/mindeyev2_dataset                     --model_name=embedsOnly                     --no-multi_subject --subj=1 --batch_size=16 --no-blurry_recon --no-depth_recon --no-clip_text --num_sessions=37                     --clip_scale=1. --blur_scale=100. --depth_scale=100. --hidden_dim=1024 --seq_len=1                     --use_prior --prior_scale=30                     --max_lr=3e-4 --mixup_pct=.50 --num_epochs=12 --ckpt_interval=1 --no-use_image_aug --no-ckpt_saving\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"embedsOnly\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --no-multi_subject --subj=1 --batch_size={batch_size} --no-blurry_recon --no-depth_recon --no-clip_text --num_sessions=37 \\\n",
    "                    --clip_scale=1. --blur_scale=100. --depth_scale=100. --hidden_dim=1024 --seq_len=1 \\\n",
    "                    --use_prior --prior_scale=30 \\\n",
    "                    --max_lr=3e-4 --mixup_pct=.50 --num_epochs=12 --ckpt_interval=1 --no-use_image_aug --no-ckpt_saving\"# --wandb_log\" #--resume_from_ckpt \n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [1] num_sessions 37\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/weka/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--visualize_prior\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"output visualizations from unCLIP every ckpt_interval (requires more memory!)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output depth reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_text\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to contrastively learn with clip text\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--depth_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from depth recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=1,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1028,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.3),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,9)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list, \"num_sessions\", num_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "### Creating wds dataloader, preload betas and all 73k possible images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing batch size by subj_list, which will then be concatenated across subj during training...\n",
      "batch_size = 16 num_iterations_per_epoch = 216 num_samples_per_epoch = 3468\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "num_voxels_list = []\n",
    "# nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])-3 # 3 sessions are withheld for algonauts\n",
    "\n",
    "if multi_subject:\n",
    "    nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "    num_samples_per_epoch = (750*40) // num_devices \n",
    "else:\n",
    "    num_samples_per_epoch = (750*num_sessions) // num_devices \n",
    "\n",
    "print(\"dividing batch size by subj_list, which will then be concatenated across subj during training...\") \n",
    "batch_size = batch_size // len(subj_list)\n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size*len(subj_list))\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 37 sessions\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..36}.tar\n",
      "num_voxels for subj01: 15724\n",
      "Loaded all subj train dls and betas!\n",
      "\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar\n",
      "Loaded test dl for subj1!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "train_dl = {}\n",
    "num_voxels = {}\n",
    "voxels = {}\n",
    "for s in subj_list:\n",
    "    print(f\"Training with {num_sessions} sessions\")\n",
    "    if multi_subject:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{nsessions_allsubj[s-1]-1}\" + \"}.tar\"\n",
    "    else:\n",
    "        train_url = f\"{data_path}/wds/subj0{s}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "    print(train_url)\n",
    "    \n",
    "    train_data[f'subj0{s}'] = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                        .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                        .decode(\"torch\")\\\n",
    "                        .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                        .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "    train_dl[f'subj0{s}'] = torch.utils.data.DataLoader(train_data[f'subj0{s}'], batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "    # Load hdf5 data for betas, but don't put everything into memory\n",
    "    f = h5py.File(f'{data_path}/betas_all_subj0{s}_fp32.hdf5', 'r')\n",
    "    # f = h5py.File(f'{data_path}/betas_subj0{subj}_thresholded_wholebrain.hdf5', 'r')\n",
    "    \n",
    "    betas = f['betas'][:]\n",
    "    betas = torch.Tensor(betas).to(\"cpu\").to(data_type)\n",
    "    num_voxels_list.append(betas[0].shape[-1])\n",
    "    num_voxels[f'subj0{s}'] = betas[0].shape[-1]\n",
    "    voxels[f'subj0{s}'] = betas\n",
    "    print(f\"num_voxels for subj0{s}: {num_voxels[f'subj0{s}']}\")\n",
    "\n",
    "print(\"Loaded all subj train dls and betas!\\n\")\n",
    "\n",
    "# Validate only on one subject\n",
    "if multi_subject: \n",
    "    subj = subj_list[0] # cant validate on the actual held out person so picking first in subj_list\n",
    "if not new_test: # using old test set from before full dataset released (used in original MindEye paper)\n",
    "    if subj==3:\n",
    "        num_test=2113\n",
    "    elif subj==4:\n",
    "        num_test=1985\n",
    "    elif subj==6:\n",
    "        num_test=2113\n",
    "    elif subj==8:\n",
    "        num_test=1985\n",
    "    else:\n",
    "        num_test=2770\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "elif new_test: # using larger test set from after full dataset released\n",
    "    if subj==3:\n",
    "        num_test=2371\n",
    "    elif subj==4:\n",
    "        num_test=2188\n",
    "    elif subj==6:\n",
    "        num_test=2371\n",
    "    elif subj==8:\n",
    "        num_test=2188\n",
    "    else:\n",
    "        num_test=3000\n",
    "    test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "print(test_url)\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=True, pin_memory=True)\n",
    "print(f\"Loaded test dl for subj{subj}!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ae3e61-be9a-4b5f-8611-c62359c7dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgViTBG(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(os.listdir(self.directory))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load and return the tensor at the given index.\n",
    "        Args:\n",
    "        - idx (int): Index of the tensor to be loaded.\n",
    "        \"\"\"\n",
    "        filename = f\"imgt{idx+1}_embed.pt\"\n",
    "        file_path = os.path.join(self.directory, filename)\n",
    "        # tensor = torch.load(file_path)\n",
    "        return file_path\n",
    "\n",
    "# Usage\n",
    "directory = '/weka/proj-fmri/shared/vitBG_embeds/'  # Replace with your directory path\n",
    "imgemb_dataset = imgViTBG(directory)\n",
    "\n",
    "def get_img_tensor(data, index_arr, batch_size):\n",
    "    emb_arr = []\n",
    "    for i in range(batch_size):\n",
    "        ind = index_arr[i]\n",
    "        path_emb = data[ind]\n",
    "        emb = torch.load(path_emb, map_location='cpu')\n",
    "        emb = emb.squeeze(0)\n",
    "        emb_arr.append(emb)\n",
    "    emb_tensor = torch.stack(emb_arr)\n",
    "    return emb_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c13b4b84-094c-4b5b-bace-26c155aa6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load 73k NSD images\n",
    "# f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "# images = f['images'][:]\n",
    "# images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "# print(\"Loaded all 73k possible NSD images to cpu!\", images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7f081a-3599-4ff7-b042-998c2291085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load COCO images and captions\n",
    "\n",
    "# f = h5py.File('/fsx/proj-fmri/shared/mindeyev2_dataset/trainval_coco_images_224_float16.hdf5', 'r')\n",
    "# coco_images = f['images']#[:]\n",
    "# print(\"coco_images\", coco_images.shape)\n",
    "\n",
    "# coco_ids = np.load(\"trainval_coco_ids.npy\")\n",
    "# print(\"coco_ids\", len(coco_ids))\n",
    "# captions_dict = dict(np.load(\"trainval_coco_captions_dict.npy\", allow_pickle=True).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a9c68c-c3c9-4080-bd99-067c4486dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check dataloaders are working\n",
    "\n",
    "# test_vox_indices = []\n",
    "# test_73k_images = []\n",
    "# for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "#     test_vox_indices = np.append(test_vox_indices, behav[:,0,5].cpu().numpy())\n",
    "#     test_73k_images = np.append(test_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# test_vox_indices = test_vox_indices.astype(np.int16)\n",
    "# print(test_i, (test_i+1) * num_test, len(test_vox_indices))\n",
    "# print(\"---\\n\")\n",
    "\n",
    "# train_vox_indices = []\n",
    "# train_73k_images = []\n",
    "# for train_i, (behav, past_behav, future_behav, old_behav) in enumerate(train_dl):\n",
    "#     train_vox_indices = np.append(train_vox_indices, behav[:,0,5].long().cpu().numpy())\n",
    "#     train_73k_images = np.append(train_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# train_vox_indices = train_vox_indices.astype(np.int16)\n",
    "# print(train_i, (train_i+1) * batch_size, len(train_vox_indices))\n",
    "\n",
    "# all_vox_indices = np.hstack((train_vox_indices, test_vox_indices))\n",
    "# all_images = np.hstack((train_73k_images, test_73k_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "#     arch=\"ViT-bigG-14\",\n",
    "#     version=\"laion2b_s39b_b160k\",\n",
    "#     output_tokens=True,\n",
    "#     only_tokens=True,\n",
    "# )\n",
    "# clip_img_embedder.to(device)\n",
    "\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "if clip_text:\n",
    "    tokenizer = get_tokenizer('ViT-H-14')\n",
    "    hookT = Hook(clip_model.transformer.resblocks[-1].ln_2)\n",
    "    def get_clip_text_embeddings(text):\n",
    "        tokens = tokenizer(text, context_length=clip_model.context_length).to(device)\n",
    "        clip_model.encode_text(tokens)\n",
    "        return hookT.outputs.permute(1,0,2)\n",
    "    clip_text_seq_dim = 77\n",
    "    clip_text_emb_dim = 1024\n",
    "    annots = np.load(\"/fsx/proj-fmri/shared/mindeyev2_dataset/COCO_73k_annots_curated.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if blurry_recon:\n",
    "#     from diffusers import AutoencoderKL\n",
    "#     autoenc = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, cache_dir=\"/fsx/proj-fmri/shared/cache\")\n",
    "#     # autoenc.load_state_dict(torch.load('../train_logs/sdxl_vae_normed/best.pth')[\"model_state_dict\"])\n",
    "#     autoenc.eval()\n",
    "#     autoenc.requires_grad_(False)\n",
    "#     autoenc.to(device)\n",
    "#     utils.count_params(autoenc)\n",
    "\n",
    "if blurry_recon:\n",
    "    # from diffusers import VQModel\n",
    "    from diffusers import VQDiffusionPipeline\n",
    "    autoenc = VQDiffusionPipeline.from_pretrained(\"microsoft/vq-diffusion-ithq\", torch_dtype=data_type, cache_dir=\"/fsx/proj-fmri/shared/cache\")\n",
    "\n",
    "    # autoenc = VQModel.from_pretrained(\"/fsx/proj-fmri/shared/cache/models--microsoft--vq-diffusion-ithq/snapshots/3f796fb49ee559370dc638dea1d8116af131d993/vqvae\", torch_dtype=data_type)\n",
    "    autoenc = autoenc.vqvae\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c8eee-9834-437d-bb60-b38faef50138",
   "metadata": {},
   "source": [
    "#### downsampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1ba8dd-64c2-4ac9-947e-725b7f2e3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if blurry_recon:\n",
    "#     if utils.is_interactive(): display(utils.torch_to_Image(images[[30]]))\n",
    "\n",
    "#     input_batch = images[[30]].to(device)\n",
    "#     print(input_batch.shape)\n",
    "\n",
    "#     downsampled_image = nn.functional.interpolate(input_batch, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "#     re_upsampled_image = nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest')\n",
    "#     re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "#     print(re_upsampled_enc.shape)\n",
    "    \n",
    "#     if utils.is_interactive(): display(utils.torch_to_Image((autoenc.decode(re_upsampled_enc/0.18215).sample / 2 + 0.5).clamp(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6390a3a8-2bef-4e81-9b82-e154d26a1e1d",
   "metadata": {},
   "source": [
    "#### MiDaS depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35573e2-95bf-463d-8937-68ad4c2c3c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if depth_recon:\n",
    "    from controlnet_aux.midas import MidasDetector\n",
    "    \n",
    "    midas_depth = MidasDetector.from_pretrained(\n",
    "      \"valhalla/t2iadapter-aux-models\", filename=\"dpt_large_384.pt\", model_type=\"dpt_large\", cache_dir=\"/fsx/proj-fmri/shared/cache\").to(device)\n",
    "    midas_depth.model.eval()\n",
    "    midas_depth.model.requires_grad_(False)\n",
    "    midas_depth.model.to(device)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba3f9207-b98e-45da-baa6-5cfcfb2ae958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if depth_recon:\n",
    "    if utils.is_interactive(): display(utils.torch_to_Image(images[[30]]))\n",
    "\n",
    "    input_batch = images[[30,31]].float().to(device)\n",
    "    print(input_batch.shape)\n",
    "    \n",
    "    midas_emb = midas_depth.model(input_batch).unsqueeze(1)\n",
    "    print(midas_emb.shape)\n",
    "\n",
    "    prediction = utils.resize(midas_emb, 32) #/30).clamp(0,1).half() # 30 is roughly prediction.max()\n",
    "    print(prediction.shape)\n",
    "    \n",
    "    prediction = (prediction / prediction.view(prediction.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(prediction)).half()\n",
    "    midas_emb_size = prediction.flatten(1).shape[1]\n",
    "    print(\"midas_emb\", prediction.shape, prediction.min(), prediction.max())\n",
    "    print(\"midas_emb_size\", midas_emb_size)\n",
    "    \n",
    "    if utils.is_interactive(): display(utils.torch_to_Image(utils.resize(prediction, 224))) \n",
    "\n",
    "    if blurry_recon:\n",
    "        prediction = utils.resize(midas_emb, 128).half().repeat(1,3,1,1)\n",
    "        prediction = (prediction / prediction.view(prediction.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(prediction)).half()\n",
    "        prediction_enc = autoenc.encode(2*prediction-1).latents * 0.18215\n",
    "        print(\"vae midas_emb\", prediction_enc.shape, prediction_enc.min(), prediction_enc.max())\n",
    "    \n",
    "        if utils.is_interactive(): display(utils.torch_to_Image((autoenc.decode(prediction_enc/0.18215).sample / 2 + 0.5).clamp(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,402 total\n",
      "16,102,402 trainable\n",
      "param counts:\n",
      "16,102,402 total\n",
      "16,102,402 trainable\n",
      "torch.Size([2, 1, 15724]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_sizes, out_features, seq_len): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "        self.temp = nn.Parameter(torch.Tensor([5.3]))\n",
    "        self.bias = nn.Parameter(torch.Tensor([-2.]))\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = torch.cat([self.linears[subj_idx](x[:,seq]).unsqueeze(1) for seq in range(seq_len)], dim=1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegression(num_voxels_list, out_features=hidden_dim, seq_len=seq_len)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test on subject 1 with fake data\n",
    "b = torch.randn((2,seq_len,num_voxels_list[0]))\n",
    "print(b.shape, model.ridge(b,0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3602c333-d029-465c-8fb4-c3ccffdba6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "456,074,128 total\n",
      "456,074,128 trainable\n",
      "param counts:\n",
      "472,176,530 total\n",
      "472,176,530 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([1]) torch.Size([1]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, seq_len=2, h=4096, n_blocks=n_blocks, drop=.15, \n",
    "                 clip_size=768, text_clip_size=768, text_out_dim=768):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        self.text_clip_size = text_clip_size\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_ln1 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mlp(seq_len, seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_ln2 = nn.ModuleList([\n",
    "            self.ln(h) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mlp(h, h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True)\n",
    "        self.clip_proj = self.projector(clip_size, clip_size)\n",
    "        if clip_text:\n",
    "            self.clin2 = nn.Linear(h * seq_len, text_out_dim, bias=True)\n",
    "            self.clip_proj_text = self.projector(text_clip_size, text_clip_size)\n",
    "\n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Sequential(\n",
    "                nn.Linear(out_dim, 4096, bias=True),\n",
    "                nn.LayerNorm(4096),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4096, 4096))\n",
    "            self.bgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=128,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "\n",
    "        if depth_recon:\n",
    "            self.dlin1 = nn.Sequential(\n",
    "                    nn.Linear(h, midas_emb_size),\n",
    "                    nn.Sigmoid(),\n",
    "                )\n",
    "            self.dgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.dupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=1,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128, 256],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "            \n",
    "    def projector(self, in_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(in_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_dim, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, out_dim)\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def ln(self, dim):\n",
    "        return nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make empty tensors for blur and depth outputs\n",
    "        t,b,d = torch.Tensor([0.]), torch.Tensor([0.]), torch.Tensor([0.])\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x.permute(0,2,1)\n",
    "        residual2 = x\n",
    "        for ln1, block1, ln2, block2 in zip(self.mixer_ln1, self.mixer_blocks1, self.mixer_ln2, self.mixer_blocks2):\n",
    "            # Layer norm before transpose\n",
    "            x = ln1(x)\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Channel mixing\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            # Embedding mixing\n",
    "            x = ln2(x)\n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        backbone = self.clin1(x).reshape(len(x), -1, self.clip_size)\n",
    "        \n",
    "        c = self.clip_proj(backbone)\n",
    "        \n",
    "        if clip_text:\n",
    "            t = self.clin2(x)\n",
    "            t = self.clip_proj_text(t.reshape(len(t), -1, self.text_clip_size))\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = b.reshape(len(b), 256, 4, 4)\n",
    "            b = self.bgroupnorm(b)\n",
    "            b = self.bupsampler(b)\n",
    "            \n",
    "        if depth_recon:\n",
    "            d = self.dlin1(x) #.reshape(len(x), 1, 32, 32)\n",
    "            d = d.reshape(len(d), 256, 4, 4)\n",
    "            d = self.dgroupnorm(d)\n",
    "            d = self.dupsampler(d)\n",
    "        \n",
    "        return backbone, c, t, b, d\n",
    "\n",
    "if clip_text:\n",
    "    model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                              clip_size=clip_emb_dim, text_clip_size=clip_text_emb_dim,\n",
    "                              out_dim=clip_emb_dim*clip_seq_dim, text_out_dim=clip_text_emb_dim*clip_text_seq_dim) \n",
    "else:\n",
    "    model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,seq_len,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, text_, blur_, depth_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, text_.shape, blur_.shape, depth_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "732,041,746 total\n",
      "732,041,730 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = VersatileDiffusionPriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "        voxel2clip=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    # prep unCLIP\n",
    "    if visualize_prior:\n",
    "        config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "        config = OmegaConf.to_container(config, resolve=True)\n",
    "        unclip_params = config[\"model\"][\"params\"]\n",
    "        network_config = unclip_params[\"network_config\"]\n",
    "        denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "        first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "        conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "        sampler_config = unclip_params[\"sampler_config\"]\n",
    "        scale_factor = unclip_params[\"scale_factor\"]\n",
    "        disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "        offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "        first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "        sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "        diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                               denoiser_config=denoiser_config,\n",
    "                               first_stage_config=first_stage_config,\n",
    "                               conditioner_config=conditioner_config,\n",
    "                               sampler_config=sampler_config,\n",
    "                               scale_factor=scale_factor,\n",
    "                               disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "        # set to inference\n",
    "        diffusion_engine.eval().requires_grad_(False)\n",
    "        diffusion_engine.to(device)\n",
    "\n",
    "        ckpt_path = '/fsx/proj-fmri/shared/mindeyev2_dataset/unclip6_epoch0_step110000.ckpt'\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "        image = images[:1].to(device)\n",
    "        batch={\"jpg\": image,\n",
    "              \"original_size_as_tuple\": torch.ones(image.shape[0], 2).to(device) * image.shape[-1],\n",
    "              \"crop_coords_top_left\": torch.zeros(image.shape[0], 2).to(device)}\n",
    "        out = diffusion_engine.conditioner(batch)\n",
    "        vector_suffix = out[\"vector\"].to(device)\n",
    "        print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 2592\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "732,041,746 total\n",
      "732,041,730 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "if use_prior:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "else:\n",
    "    opt_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "# def save_ckpt(tag):\n",
    "#     if use_deepspeed:\n",
    "#         deepspeed.DeepSpeedEngine.save_checkpoint(model, save_dir=outdir, tag=tag)\n",
    "#         ckpt_path = outdir+f'/{tag}/{tag}.npy'\n",
    "#         np.save(ckpt_path, {\n",
    "#             'epoch': epoch,\n",
    "#             'train_losses': losses,\n",
    "#             'test_losses': test_losses,\n",
    "#             'lrs': lrs})\n",
    "#     else:\n",
    "#         ckpt_path = outdir+f'/{tag}.pth'\n",
    "#         unwrapped_model = accelerator.unwrap_model(model)\n",
    "#         torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': unwrapped_model.state_dict(),\n",
    "#             'optimizer_state_dict': optimizer.state_dict(),\n",
    "#             'lr_scheduler': lr_scheduler.state_dict(),\n",
    "#             'train_losses': losses,\n",
    "#             'test_losses': test_losses,\n",
    "#             'lrs': lrs,\n",
    "#             }, ckpt_path)\n",
    "#         del unwrapped_model\n",
    "#     print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "# def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True): \n",
    "#     print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "#     if use_deepspeed:\n",
    "#         state_dict = deepspeed.utils.zero_to_fp32.get_fp32_state_dict_from_zero_checkpoint(checkpoint_dir=outdir, tag=tag)\n",
    "#         try:\n",
    "#             model.module.load_state_dict(state_dict, strict=strict)\n",
    "#         except:\n",
    "#             model.load_state_dict(state_dict, strict=strict)\n",
    "#         if load_epoch:\n",
    "#             np_ckpt = np.load(outdir+f'/{tag}/{tag}.npy', allow_pickle=True).tolist()\n",
    "#             globals()[\"epoch\"] = np_ckpt['epoch']\n",
    "#             print(\"Epoch\",epoch)\n",
    "#     else:\n",
    "#         checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "#         if load_epoch:\n",
    "#             globals()[\"epoch\"] = checkpoint['epoch']\n",
    "#             print(\"Epoch\",epoch)\n",
    "#         if load_optimizer:\n",
    "#             optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#         if load_lr:\n",
    "#             lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "#         try:\n",
    "#             model.module.load_state_dict(state_dict, strict=strict)\n",
    "#         except:\n",
    "#             model.load_state_dict(state_dict, strict=strict)\n",
    "#         del checkpoint\n",
    "        \n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2ca37a-36c5-403a-b7b3-b3b0491c47b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_log = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'diffuserEmbeds'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5453c316-0cb0-4bee-8585-f44dff746e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load saved ckpt model weights into current model\n",
    "# if resume_from_ckpt:\n",
    "#     load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)\n",
    "# elif wandb_log:\n",
    "#     if wandb.run.resumed:\n",
    "#         load_ckpt(\"last\",load_lr=True,load_optimizer=True,load_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /admin/home-mihirneal/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /admin/home-mihirneal/.cache/torch_extensions/py310_cu121/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 3.159623622894287 seconds\n",
      "[2024-01-04 06:22:42,495] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.6, git-hash=unknown, git-branch=unknown\n",
      "[2024-01-04 06:22:44,468] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-01-04 06:22:44,470] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-01-04 06:22:44,471] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-01-04 06:22:44,474] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-01-04 06:22:44,474] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-01-04 06:22:44,475] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2024-01-04 06:22:44,475] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 10000000\n",
      "[2024-01-04 06:22:44,475] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000\n",
      "[2024-01-04 06:22:44,475] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: True\n",
      "[2024-01-04 06:22:44,476] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False\n",
      "[2024-01-04 06:22:48,115] [INFO] [utils.py:791:see_memory_usage] Before initializing optimizer states\n",
      "[2024-01-04 06:22:48,117] [INFO] [utils.py:792:see_memory_usage] MA 2.18 GB         Max_MA 2.18 GB         CA 2.18 GB         Max_CA 2 GB \n",
      "[2024-01-04 06:22:48,117] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 51.51 GB, percent = 4.6%\n",
      "[2024-01-04 06:22:50,588] [INFO] [utils.py:791:see_memory_usage] After initializing optimizer states\n",
      "[2024-01-04 06:22:50,590] [INFO] [utils.py:792:see_memory_usage] MA 2.18 GB         Max_MA 2.18 GB         CA 2.18 GB         Max_CA 2 GB \n",
      "[2024-01-04 06:22:50,590] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 60.09 GB, percent = 5.4%\n",
      "[2024-01-04 06:22:50,591] [INFO] [stage_1_and_2.py:516:__init__] optimizer state initialized\n",
      "[2024-01-04 06:22:50,778] [INFO] [utils.py:791:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-01-04 06:22:50,779] [INFO] [utils.py:792:see_memory_usage] MA 2.18 GB         Max_MA 2.18 GB         CA 2.18 GB         Max_CA 2 GB \n",
      "[2024-01-04 06:22:50,780] [INFO] [utils.py:799:see_memory_usage] CPU Virtual Memory:  used = 60.09 GB, percent = 5.4%\n",
      "[2024-01-04 06:22:50,787] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedCPUAdam\n",
      "[2024-01-04 06:22:50,787] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-01-04 06:22:50,788] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.OneCycleLR object at 0x7f5ad3be6c50>\n",
      "[2024-01-04 06:22:50,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05, 1.200000000000002e-05], mom=[(0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999), (0.95, 0.999)]\n",
      "[2024-01-04 06:22:50,789] [INFO] [config.py:984:print] DeepSpeedEngine configuration:\n",
      "[2024-01-04 06:22:50,789] [INFO] [config.py:988:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-01-04 06:22:50,790] [INFO] [config.py:988:print]   aio_config ................... {'block_size': 26214400, 'queue_depth': 32, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-01-04 06:22:50,790] [INFO] [config.py:988:print]   amp_enabled .................. False\n",
      "[2024-01-04 06:22:50,790] [INFO] [config.py:988:print]   amp_params ................... False\n",
      "[2024-01-04 06:22:50,791] [INFO] [config.py:988:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-01-04 06:22:50,791] [INFO] [config.py:988:print]   bfloat16_enabled ............. False\n",
      "[2024-01-04 06:22:50,791] [INFO] [config.py:988:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-01-04 06:22:50,792] [INFO] [config.py:988:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-01-04 06:22:50,792] [INFO] [config.py:988:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-01-04 06:22:50,792] [INFO] [config.py:988:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5b78ddc100>\n",
      "[2024-01-04 06:22:50,792] [INFO] [config.py:988:print]   communication_data_type ...... None\n",
      "[2024-01-04 06:22:50,793] [INFO] [config.py:988:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-01-04 06:22:50,793] [INFO] [config.py:988:print]   curriculum_enabled_legacy .... False\n",
      "[2024-01-04 06:22:50,793] [INFO] [config.py:988:print]   curriculum_params_legacy ..... False\n",
      "[2024-01-04 06:22:50,794] [INFO] [config.py:988:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-01-04 06:22:50,794] [INFO] [config.py:988:print]   data_efficiency_enabled ...... False\n",
      "[2024-01-04 06:22:50,794] [INFO] [config.py:988:print]   dataloader_drop_last ......... False\n",
      "[2024-01-04 06:22:50,794] [INFO] [config.py:988:print]   disable_allgather ............ False\n",
      "[2024-01-04 06:22:50,795] [INFO] [config.py:988:print]   dump_state ................... False\n",
      "[2024-01-04 06:22:50,795] [INFO] [config.py:988:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-01-04 06:22:50,795] [INFO] [config.py:988:print]   eigenvalue_enabled ........... False\n",
      "[2024-01-04 06:22:50,795] [INFO] [config.py:988:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-01-04 06:22:50,796] [INFO] [config.py:988:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-01-04 06:22:50,796] [INFO] [config.py:988:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-01-04 06:22:50,796] [INFO] [config.py:988:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-01-04 06:22:50,797] [INFO] [config.py:988:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-01-04 06:22:50,797] [INFO] [config.py:988:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-01-04 06:22:50,797] [INFO] [config.py:988:print]   eigenvalue_verbose ........... False\n",
      "[2024-01-04 06:22:50,797] [INFO] [config.py:988:print]   elasticity_enabled ........... False\n",
      "[2024-01-04 06:22:50,798] [INFO] [config.py:988:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-01-04 06:22:50,798] [INFO] [config.py:988:print]   fp16_auto_cast ............... False\n",
      "[2024-01-04 06:22:50,798] [INFO] [config.py:988:print]   fp16_enabled ................. True\n",
      "[2024-01-04 06:22:50,798] [INFO] [config.py:988:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-01-04 06:22:50,799] [INFO] [config.py:988:print]   global_rank .................. 0\n",
      "[2024-01-04 06:22:50,799] [INFO] [config.py:988:print]   grad_accum_dtype ............. None\n",
      "[2024-01-04 06:22:50,799] [INFO] [config.py:988:print]   gradient_accumulation_steps .. 1\n",
      "[2024-01-04 06:22:50,800] [INFO] [config.py:988:print]   gradient_clipping ............ 1.0\n",
      "[2024-01-04 06:22:50,800] [INFO] [config.py:988:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-01-04 06:22:50,800] [INFO] [config.py:988:print]   graph_harvesting ............. False\n",
      "[2024-01-04 06:22:50,800] [INFO] [config.py:988:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-01-04 06:22:50,801] [INFO] [config.py:988:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-01-04 06:22:50,801] [INFO] [config.py:988:print]   load_universal_checkpoint .... False\n",
      "[2024-01-04 06:22:50,801] [INFO] [config.py:988:print]   loss_scale ................... 0\n",
      "[2024-01-04 06:22:50,801] [INFO] [config.py:988:print]   memory_breakdown ............. False\n",
      "[2024-01-04 06:22:50,802] [INFO] [config.py:988:print]   mics_hierarchial_params_gather  False\n",
      "[2024-01-04 06:22:50,802] [INFO] [config.py:988:print]   mics_shard_size .............. -1\n",
      "[2024-01-04 06:22:50,802] [INFO] [config.py:988:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-01-04 06:22:50,802] [INFO] [config.py:988:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-01-04 06:22:50,803] [INFO] [config.py:988:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-01-04 06:22:50,803] [INFO] [config.py:988:print]   optimizer_name ............... None\n",
      "[2024-01-04 06:22:50,803] [INFO] [config.py:988:print]   optimizer_params ............. None\n",
      "[2024-01-04 06:22:50,804] [INFO] [config.py:988:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-01-04 06:22:50,804] [INFO] [config.py:988:print]   pld_enabled .................. False\n",
      "[2024-01-04 06:22:50,804] [INFO] [config.py:988:print]   pld_params ................... False\n",
      "[2024-01-04 06:22:50,804] [INFO] [config.py:988:print]   prescale_gradients ........... False\n",
      "[2024-01-04 06:22:50,805] [INFO] [config.py:988:print]   scheduler_name ............... None\n",
      "[2024-01-04 06:22:50,805] [INFO] [config.py:988:print]   scheduler_params ............. None\n",
      "[2024-01-04 06:22:50,805] [INFO] [config.py:988:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-01-04 06:22:50,805] [INFO] [config.py:988:print]   sparse_attention ............. None\n",
      "[2024-01-04 06:22:50,806] [INFO] [config.py:988:print]   sparse_gradients_enabled ..... False\n",
      "[2024-01-04 06:22:50,806] [INFO] [config.py:988:print]   steps_per_print .............. inf\n",
      "[2024-01-04 06:22:50,806] [INFO] [config.py:988:print]   train_batch_size ............. 16\n",
      "[2024-01-04 06:22:50,806] [INFO] [config.py:988:print]   train_micro_batch_size_per_gpu  16\n",
      "[2024-01-04 06:22:50,807] [INFO] [config.py:988:print]   use_data_before_expert_parallel_  False\n",
      "[2024-01-04 06:22:50,807] [INFO] [config.py:988:print]   use_node_local_storage ....... False\n",
      "[2024-01-04 06:22:50,807] [INFO] [config.py:988:print]   wall_clock_breakdown ......... False\n",
      "[2024-01-04 06:22:50,807] [INFO] [config.py:988:print]   weight_quantization_config ... None\n",
      "[2024-01-04 06:22:50,808] [INFO] [config.py:988:print]   world_size ................... 1\n",
      "[2024-01-04 06:22:50,808] [INFO] [config.py:988:print]   zero_allow_untested_optimizer  True\n",
      "[2024-01-04 06:22:50,808] [INFO] [config.py:988:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=10000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=PosixPath('/scratch'), buffer_count=5, buffer_size=4000000000, max_in_cpu=1,000,000,000, pin_memory=True) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=PosixPath('/scratch'), buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=10000000 param_persistence_threshold=100000 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-01-04 06:22:50,809] [INFO] [config.py:988:print]   zero_enabled ................. True\n",
      "[2024-01-04 06:22:50,809] [INFO] [config.py:988:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-01-04 06:22:50,809] [INFO] [config.py:988:print]   zero_optimization_stage ...... 2\n",
      "[2024-01-04 06:22:50,810] [INFO] [config.py:974:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_prefetch_bucket_size\": 1.000000e+07, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+05, \n",
      "        \"reduce_bucket_size\": 1.000000e+07, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\", \n",
      "            \"nvme_path\": \"/scratch\", \n",
      "            \"buffer_size\": 4.000000e+09, \n",
      "            \"pin_memory\": true\n",
      "        }\n",
      "    }, \n",
      "    \"aio\": {\n",
      "        \"block_size\": 2.621440e+07, \n",
      "        \"queue_depth\": 32, \n",
      "        \"thread_count\": 1, \n",
      "        \"single_submit\": false, \n",
      "        \"overlap_events\": true\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 16, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "train_dls = [train_dl[f'subj0{s}'] for s in subj_list]\n",
    "\n",
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedsOnly starting with epoch 0 / 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000300, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2024-01-04 06:23:23,636] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-mihirneal/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1947: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "  overflow_gpu = get_accelerator().ByteTensor([overflow])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-04 06:23:24,183] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2024-01-04 06:23:24,730] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2024-01-04 06:23:25,275] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2024-01-04 06:23:25,821] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2024-01-04 06:23:26,368] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2024-01-04 06:23:26,914] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2024-01-04 06:23:27,459] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2024-01-04 06:23:28,005] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2024-01-04 06:23:28,553] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2024-01-04 06:23:29,099] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2024-01-04 06:23:29,648] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2024-01-04 06:23:30,196] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2024-01-04 06:23:30,743] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2024-01-04 06:23:31,290] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2024-01-04 06:23:31,837] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2024-01-04 06:23:32,387] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2024-01-04 06:23:32,934] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-01-04 06:23:33,482] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2024-01-04 06:23:34,030] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/12 [00:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 239\u001b[0m\n\u001b[1;32m    236\u001b[0m         depth_pixcorr \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pixcorr\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    238\u001b[0m utils\u001b[38;5;241m.\u001b[39mcheck_loss(loss)\n\u001b[0;32m--> 239\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    242\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/accelerate/accelerator.py:1899\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepspeed_engine_wrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1900\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mMEGATRON_LM:\n\u001b[1;32m   1901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/accelerate/utils/deepspeed.py:176\u001b[0m, in \u001b[0;36mDeepSpeedEngineWrapper.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# Deepspeed's `engine.step` performs the following operations:\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# - gradient accumulation check\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# - gradient clipping\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# - checking overflow\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# - lr_scheduler step (only if engine.lr_scheduler is not None)\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/engine.py:2148\u001b[0m, in \u001b[0;36mDeepSpeedEngine.step\u001b[0;34m(self, lr_kwargs)\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_take_model_step(lr_kwargs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_eigenvalue)\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2148\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_model_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2150\u001b[0m     report_progress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_rank \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtput_timer\u001b[38;5;241m.\u001b[39mstop(global_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_gradient_accumulation_boundary(), report_speed\u001b[38;5;241m=\u001b[39mreport_progress)\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/engine.py:2054\u001b[0m, in \u001b[0;36mDeepSpeedEngine._take_model_step\u001b[0;34m(self, lr_kwargs, block_eigenvalue)\u001b[0m\n\u001b[1;32m   2052\u001b[0m         master_params \u001b[38;5;241m=\u001b[39m amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer)\n\u001b[1;32m   2053\u001b[0m         clip_grad_norm_(parameters\u001b[38;5;241m=\u001b[39mmaster_params, max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_clipping(), mpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmpu)\n\u001b[0;32m-> 2054\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_global_grad_norm\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   2057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39m_global_grad_norm\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1820\u001b[0m, in \u001b[0;36mDeepSpeedZeroOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m   1818\u001b[0m     bit16_partitions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_partitioned_bit16_groups[i]\n\u001b[1;32m   1819\u001b[0m     fp32_partition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingle_partition_of_fp32_groups[i]\n\u001b[0;32m-> 1820\u001b[0m     \u001b[43mbit16_partitions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpartition_id\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp32_partition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimers(OPTIMIZER_STEP_TIMER)\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;66;03m# free gradients for all the parameters that are not updated by this process(ZeRO stage2)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "skip_train = True if epoch>=(num_epochs-1) else False # skip training if you are resuming from a fully trained model\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "\n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "    \n",
    "    fwd_text_percent_correct = 0.\n",
    "    bwd_text_percent_correct = 0.\n",
    "    test_fwd_text_percent_correct = 0.\n",
    "    test_bwd_text_percent_correct = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_depth_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    test_loss_blurry_total = 0.\n",
    "    test_loss_depth_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "\n",
    "    depth_pixcorr = 0.\n",
    "    test_depth_pixcorr = 0.\n",
    "\n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    voxel_iters = {} # empty dict because diff subjects have differing # of voxels\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 256, 1664).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            for iter, (behav0, past_behav0, future_behav0, old_behav0) in enumerate(train_dl):\n",
    "                arr = behav0[:,0,0].cpu().long()\n",
    "                image0 = behav0[:,0,0].cpu().long()\n",
    "                emb = get_img_tensor(imgemb_dataset, image0, batch_size).half().to(device)\n",
    "                image_iters[iter,s*batch_size:s*batch_size+batch_size] = emb\n",
    "                \n",
    "                if clip_text:\n",
    "                    annot_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = utils.select_annotations(annots[behav0[:,0,0].cpu().long()])\n",
    "\n",
    "                voxel0 = voxels[f'subj0{subj_list[s]}'][behav0[:,0,5].cpu().long()]\n",
    "                voxel0 = torch.Tensor(voxel0)\n",
    "\n",
    "                past_behavior = past_behav0[:,:(seq_len-1),5].cpu().long()\n",
    "                past_voxel0 = voxels[f'subj0{subj_list[s]}'][past_behavior]\n",
    "                past_voxel0[past_behavior==-1] = voxel0[torch.where(past_behavior==-1)[0]] # replace invalid past voxels \n",
    "                past_voxel0 = torch.Tensor(past_voxel0)\n",
    "                # # if shared100, then you need to mask it out \n",
    "                # for p in range(seq_len-1):\n",
    "                #     if past_behav[:,p,-1] == 1: \n",
    "                #         past_voxels[p] = torch.zeros_like(past_voxels[p])\n",
    "\n",
    "                voxel0 = torch.cat((voxel0.unsqueeze(1), past_voxel0), axis=1)\n",
    "                # voxel0 = torch.hstack((voxel0, past_voxel0.flatten(1))).unsqueeze(1)\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    voxel0, perm, betas, select = utils.mixco(voxel0)\n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "\n",
    "                voxel_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = voxel0\n",
    "\n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "    # you now have voxel_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    \n",
    "    if skip_train is False:\n",
    "        for train_i in range(num_iterations_per_epoch):\n",
    "            with torch.cuda.amp.autocast(dtype=data_type):\n",
    "                optimizer.zero_grad()\n",
    "                loss=0.\n",
    "\n",
    "                voxel_list = [voxel_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                imgemb_list = image_iters[train_i]\n",
    "                i = batch_size * len(subj_list)\n",
    "                assert imgemb_list.shape == torch.Size([i, 256, 1664])\n",
    "                # image = image_iters[train_i].detach().to(device)\n",
    "                if clip_text:\n",
    "                    annot = [annot_iters[f\"subj0{s}_iter{train_i}\"] for s in subj_list]\n",
    "\n",
    "                # if not epoch < int(mixup_pct * num_epochs):\n",
    "                #     extra_image = coco_images[np.random.choice(len(coco_images), batch_size, replace=False)].to(device).float()\n",
    "                #     image = torch.vstack((image, extra_image))\n",
    "\n",
    "                if blurry_recon:\n",
    "                    ran = np.random.rand()\n",
    "                    if ran > .66:\n",
    "                        blurry_image = utils.resize(transforms.GaussianBlur(kernel_size=(15,15),sigma=(12,12))(image), 128)\n",
    "                        # utils.resize(nn.functional.interpolate(image, size=(4, 4), mode='bilinear', align_corners=False),128)\n",
    "                    elif ran > .33:\n",
    "                        blurry_image = utils.resize(transforms.GaussianBlur(kernel_size=(115,115),sigma=(112,112))(image), 128)\n",
    "                        # utils.resize(nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False),128)\n",
    "                    else:\n",
    "                        blurry_image = utils.resize(nn.functional.interpolate(image, size=(12, 12), mode='bilinear', align_corners=False), 128)\n",
    "\n",
    "                    blurry_image_enc = autoenc.encode(2*blurry_image-1).latents * 0.18215\n",
    "\n",
    "                if depth_recon:\n",
    "                    # depth_images = utils.resize(midas_depth.model(image).unsqueeze(1).repeat(1,3,1,1), 128)\n",
    "                    depth_images = utils.resize(midas_depth.model(image).unsqueeze(1), 32) # batch x 1 x 32 x 32\n",
    "                    depth_images = (depth_images / depth_images.view(depth_images.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(depth_images)).half()\n",
    "                    # depth_images = nn.functional.interpolate(depth_images, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    depth_image = depth_images # autoenc.encode(2*depth_images-1).latents * 0.18215\n",
    "\n",
    "                # if use_image_aug: \n",
    "                #     image = img_augment(image)\n",
    "\n",
    "                clip_target = imgemb_list\n",
    "                clip_target = clip_target.to(device)\n",
    "                if clip_text: clip_text_target = get_clip_text_embeddings(annot[0])\n",
    "                assert not torch.any(torch.isnan(clip_target))\n",
    "\n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    perm = torch.cat(perm_list, dim=0)\n",
    "                    betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    betas = torch.cat(betas_list, dim=0)\n",
    "                    select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                    select = torch.cat(select_list, dim=0)\n",
    "\n",
    "                voxel_ridge_list = [model.ridge(voxel_list[si],si) for si,s in enumerate(subj_list)]\n",
    "                voxel_ridge = torch.cat(voxel_ridge_list, dim=0)\n",
    "\n",
    "                backbone, clip_voxels, clip_text_voxels, blurry_image_enc_, depth_image_ = model.backbone(voxel_ridge)\n",
    "                # backbone = utils.prep_for_prior(backbone)\n",
    "\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if use_prior:\n",
    "                    # clip_target_prior = utils.prep_for_prior(clip_target)\n",
    "                    loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    recon_mse += mse(prior_out, clip_target).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    clip_text_voxels_norm = nn.functional.normalize(clip_text_voxels.flatten(1), dim=-1)\n",
    "                    clip_text_target_norm = nn.functional.normalize(clip_text_target.flatten(1), dim=-1)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    if epoch < int(mixup_pct * num_epochs):                \n",
    "                        loss_clip = utils.mixco_nce(\n",
    "                            clip_voxels_norm,\n",
    "                            clip_target_norm,\n",
    "                            temp=.006,\n",
    "                            perm=perm, betas=betas, select=select)\n",
    "                        if clip_text:\n",
    "                            loss_clip += utils.mixco_nce(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=.006,\n",
    "                                perm=perm, betas=betas, select=select)\n",
    "                    else:\n",
    "                        epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                        loss_clip = utils.soft_clip_loss(\n",
    "                            clip_voxels_norm,\n",
    "                            clip_target_norm,\n",
    "                            temp=epoch_temp)\n",
    "                        if clip_text:\n",
    "                            loss_clip += utils.soft_clip_loss(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=epoch_temp)\n",
    "\n",
    "                    loss_clip_total += loss_clip.item()\n",
    "                    loss_clip *= clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    # downsampled_image = nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    # re_upsampled_image = utils.add_saturation(nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest'))\n",
    "                    # re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc) #+ l1(blurry_image_enc_, re_upsampled_enc))\n",
    "                    # loss_blurry += l1(torch.var(blurry_image_enc), torch.var(blurry_image_enc_))\n",
    "                    # loss_blurry -= compute_negative_l1_losses(blurry_image_enc_.flatten(1), blurry_image_enc.flatten(1)) * 1e-5\n",
    "                    loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "\n",
    "                if depth_recon:\n",
    "                    loss_depth = l1(depth_image_, depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(depth_image))\n",
    "                    # quantized_depth_image = torch.round(depth_image * 5) / 5\n",
    "                    # loss_depth = l1(depth_image_, quantized_depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(quantized_depth_image))\n",
    "                    # loss_depth -= compute_negative_l1_losses(depth_image_.flatten(1), depth_image.flatten(1)) * 1e-5\n",
    "                    loss_depth_total += loss_depth.item()\n",
    "                    loss_depth *= depth_scale\n",
    "                    loss += loss_depth\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    fwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_voxels_norm, clip_text_target_norm), labels, k=1).item()\n",
    "                    bwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_target_norm, clip_text_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                if blurry_recon:\n",
    "                    with torch.no_grad():\n",
    "                        # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                        random_samps = np.random.choice(np.arange(len(image)), size=batch_size//5, replace=False)\n",
    "                        blurry_recon_images = (autoenc.decode(blurry_image_enc_[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                        pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                        blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if depth_recon:\n",
    "                    with torch.no_grad():\n",
    "                        pixcorr = utils.pixcorr(depth_image, depth_image_)\n",
    "                        depth_pixcorr += pixcorr.item()\n",
    "\n",
    "                utils.check_loss(loss)\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "\n",
    "                losses.append(loss.item())\n",
    "                lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                if lr_scheduler_type is not None:\n",
    "                    lr_scheduler.step()\n",
    "                    \n",
    "    print(\"Starting Evals\")\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "\n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[f'subj0{subj}'][behav[:,0,5].cpu().long()]\n",
    "                    \n",
    "                    past_behavior = past_behav[:,:(seq_len-1),5].cpu().long()\n",
    "                    past_voxels = voxels[f'subj0{subj}'][past_behavior]\n",
    "                    \n",
    "                    if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                        past_voxels[torch.where(past_behavior==-1)[0]] = 0\n",
    "\n",
    "                    voxel = torch.cat((voxel.unsqueeze(1), past_voxels), axis=1)\n",
    "                    # voxel = torch.hstack((voxel, past_voxels.flatten(1))).unsqueeze(1)\n",
    "\n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "\n",
    "\n",
    "                    unique_image, sort_indices = torch.unique(image, return_inverse=True)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            im = im.item()\n",
    "                            emb_tensor = get_img_tensor(imgemb_dataset, [im], 1)\n",
    "                            test_image = emb_tensor[None]\n",
    "                            #test_voxel = torch.mean(voxel[locs],axis=0)[None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                            # if seq_len > 1:\n",
    "                            #     test_past_voxel = past_voxels[locs][None]\n",
    "                            if clip_text: test_annot = utils.select_annotations(annots[[im]])\n",
    "                        else:\n",
    "                            im = im.item()\n",
    "                            emb_tensor = get_img_tensor(imgemb_dataset, [im], 1)\n",
    "                            test_image = torch.vstack((test_image, emb_tensor[None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "                            # if seq_len > 1:\n",
    "                            #     test_past_voxel = torch.vstack((test_past_voxel, past_voxels[locs][None]))\n",
    "                            if clip_text: test_annot = np.vstack((test_annot,utils.select_annotations(annots[[im]])))\n",
    "\n",
    "                loss=0.\n",
    "                            \n",
    "                test_indices = torch.arange(len(test_voxel))[:300]\n",
    "                voxel = test_voxel[test_indices].to(device)\n",
    "                # if seq_len > 1: \n",
    "                #     past_voxel = test_past_voxel[test_indices].to(device)\n",
    "                image = test_image[test_indices].to(device)\n",
    "                if clip_text: annot = test_annot[test_indices]\n",
    "                assert len(image) == 300\n",
    "\n",
    "                if blurry_recon:\n",
    "                    blurry_image_enc = autoenc.encode(2*utils.resize(image,128)-1).latents * 0.18215\n",
    "\n",
    "                if depth_recon:\n",
    "                    depth_images = utils.resize(midas_depth.model(image).unsqueeze(1), 32)\n",
    "                    depth_images = (depth_images / depth_images.view(depth_images.shape[0], -1).max(dim=1)[0].view(-1, 1, 1, 1).expand_as(depth_images)).half()\n",
    "                    # depth_images = nn.functional.interpolate(depth_images, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    depth_image = depth_images\n",
    "\n",
    "                # clip_target = clip_img_embedder(image.float())\n",
    "                if clip_text: clip_text_target = get_clip_text_embeddings(annot.flatten())\n",
    "\n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep],0) # 0th index of subj_list\n",
    "                    # if seq_len > 1:\n",
    "                    #     past_voxel_ridge = model.ridge(past_voxel[:,rep],0)\n",
    "                    #     voxel_ridge = torch.cat((voxel_ridge, past_voxel_ridge), axis=1)\n",
    "                    backbone0, clip_voxels0, clip_text_voxels, blurry_image_enc_, depth_image_ = model.backbone(voxel_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        backbone += backbone0\n",
    "                clip_voxels /= 3\n",
    "                backbone /= 3\n",
    "                # backbone = utils.prep_for_prior(backbone)\n",
    "                clip_target = get_img_tensor(imgemb_dataset, test_indices, 300)\n",
    "                clip_target = clip_target.to(device)\n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                if use_prior:\n",
    "                    # clip_target_prior = utils.prep_for_prior(clip_target)\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    \n",
    "                    # now get unCLIP prediction without feeding it the image embed to get uncontaminated reconstruction\n",
    "                    prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                                    text_cond = dict(text_embed = backbone), \n",
    "                                    cond_scale = 1., timesteps = timesteps)\n",
    "                    \n",
    "                    test_recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                    test_recon_mse += mse(prior_out, clip_target).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    clip_text_voxels_norm = nn.functional.normalize(clip_text_voxels.flatten(1), dim=-1)\n",
    "                    clip_text_target_norm = nn.functional.normalize(clip_text_target.flatten(1), dim=-1)\n",
    "\n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_voxels_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "                    if clip_text:\n",
    "                        loss_clip_text = utils.soft_clip_loss(\n",
    "                                clip_text_voxels_norm,\n",
    "                                clip_text_target_norm,\n",
    "                                temp=.006)\n",
    "                        loss_clip += loss_clip_text\n",
    "\n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    # downsampled_image = nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "                    # re_upsampled_image = utils.add_saturation(nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest'))\n",
    "                    # re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latents * 0.18215\n",
    "\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc) #+ l1(blurry_image_enc_, re_upsampled_enc))\n",
    "                    # loss_blurry += l1(torch.var(blurry_image_enc), torch.var(blurry_image_enc_))\n",
    "                    # loss_blurry -= compute_negative_l1_losses(blurry_image_enc_.flatten(1), blurry_image_enc.flatten(1)) * 1e-5\n",
    "                    test_loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "\n",
    "                    # halving the batch size because the decoder is computationally heavy\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[:len(image)//2]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    blurry_recon_images = torch.vstack((blurry_recon_images, (autoenc.decode(blurry_image_enc_[len(image)//2:]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    pixcorr = utils.pixcorr(image, blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "                if depth_recon:\n",
    "                    loss_depth = l1(depth_image_, depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(depth_image))\n",
    "                    # quantized_depth_image = torch.round(depth_image * 5) / 5\n",
    "                    # loss_depth = l1(depth_image_, quantized_depth_image)\n",
    "                    # loss_depth += l1(torch.var(depth_image_), torch.var(quantized_depth_image))\n",
    "                    # loss_depth -= compute_negative_l1_losses(depth_image_.flatten(1), depth_image.flatten(1)) * 1e-5\n",
    "                    test_loss_depth_total += loss_depth.item()\n",
    "                    loss_depth *= depth_scale\n",
    "                    loss += loss_depth\n",
    "\n",
    "                    pixcorr = utils.pixcorr(depth_image, depth_image_)\n",
    "                    test_depth_pixcorr += pixcorr.item()\n",
    "\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_voxels_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "                \n",
    "                if clip_text:\n",
    "                    test_fwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_voxels_norm, clip_text_target_norm), labels, k=1).item()\n",
    "                    test_bwd_text_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_text_target_norm, clip_text_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            # if utils.is_interactive(): clear_output(wait=True)\n",
    "            if skip_train: break\n",
    "            print(\"---\")\n",
    "\n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/fwd_text_pct_correct\": fwd_text_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_text_pct_correct\": bwd_text_percent_correct / (train_i + 1),\n",
    "                \"test/test_text_fwd_pct_correct\": test_fwd_text_percent_correct / (test_i + 1),\n",
    "                \"test/test_text_bwd_pct_correct\": test_bwd_text_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"test/loss_blurry_total\": test_loss_blurry_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/depth_pixcorr\": depth_pixcorr / (train_i + 1),\n",
    "                \"test/depth_pixcorr\": test_depth_pixcorr / (test_i + 1),\n",
    "                \"train/loss_depth_total\": loss_depth_total / (train_i + 1),\n",
    "                \"test/loss_depth_total\": test_loss_depth_total / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "\n",
    "            if use_prior: # output recons every ckpt\n",
    "                if True:\n",
    "                    # print(\"Embedding UMAP\")\n",
    "                    prior_flat = prior_out[0].cpu()\n",
    "                    clip_target_flat = clip_target[0].cpu()\n",
    "                    assert prior_flat.shape == torch.Size([256, 1664]) and clip_target_flat.shape == torch.Size([256, 1664])\n",
    "                    combined_embeddings = np.concatenate([prior_flat, clip_target_flat])\n",
    "                    umap_model = umap.UMAP(random_seed=42)\n",
    "                    umap_projections = umap_model.fit_transform(combined_embeddings)\n",
    "                    # mean_euclidean_distance = np.mean(np.linalg.norm(backbone_proj - clip_target_proj, axis=2))\n",
    "                    euclidean_dist = euclidean(prior_flat.mean(axis=0), clip_target_flat.mean(axis=0))\n",
    "\n",
    "                    # Plotting\n",
    "                    prior_flat = umap_projections[:prior_flat.shape[0], :]\n",
    "                    clip_target_flat = umap_projections[prior_flat.shape[0]:, :]\n",
    "                    plt.scatter(prior_flat[:, 0], prior_flat[:, 1], label='Diffusion Prior', alpha=0.5)\n",
    "                    plt.scatter(clip_target_flat[:, 0], clip_target_flat[:, 1], label='VITBG Target', alpha=0.5)\n",
    "                    plt.title(f'UMAP Projection (Euclidean Distance: {euclidean_dist:.2f})')\n",
    "                    plt.legend()\n",
    "                    wandb.log({f\"UMAP Projection, iteration {epoch}\": wandb.Image(plt)})\n",
    "                    plt.close()\n",
    "\n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc_[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "\n",
    "                if depth_recon:\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    # axes[0].imshow(utils.torch_to_Image((autoenc.decode(depth_image[[0]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    # axes[1].imshow(utils.torch_to_Image((autoenc.decode(depth_image_[[0]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image(utils.resize(depth_image[[j]].view(1,1,32,32).clamp(0,1), 224)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image(utils.resize(depth_image_[[j]].view(1,1,32,32).clamp(0,1), 224)))\n",
    "                        axes[jj].axis('off')\n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/depth_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "\n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5aed4541-ea10-4b44-b23a-8c7d001b054a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7fc218f73d90> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:384\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:617\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:283\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1734, 16, 16])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7fc1093e6170> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:379\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mlog_code(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    378\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, res)\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:609\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:279\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "image_iters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19598456-23d9-47e2-9c8f-5461abfb7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._resume_backend at 0x7fc218f73d90> (for pre_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:384\u001b[0m, in \u001b[0;36m_WandbInit._resume_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresuming backend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 384\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:617\u001b[0m, in \u001b[0;36mInterfaceBase.publish_resume\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    616\u001b[0m     resume \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mResumeRequest()\n\u001b[0;32m--> 617\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:283\u001b[0m, in \u001b[0;36mInterfaceShared._publish_resume\u001b[0;34m(self, resume)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb\u001b[38;5;241m.\u001b[39mResumeRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(resume\u001b[38;5;241m=\u001b[39mresume)\n\u001b[0;32m--> 283\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|                                                                                                   | 111/1734 [04:52<1:11:20,  2.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     emb_tensor_3d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(iter_batches)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m emb_tensor_3d\n\u001b[0;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mget_img_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgemb_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 18\u001b[0m, in \u001b[0;36mget_img_tensor\u001b[0;34m(data, index_arr, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m         emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_emb, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m         sing_arr\u001b[38;5;241m.\u001b[39mappend(emb)\n\u001b[0;32m---> 18\u001b[0m     sing_ten \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43msing_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     batch_emb_arr\u001b[38;5;241m.\u001b[39mappend(sing_ten)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Stack all embeddings for this iteration into a single tensor\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _WandbInit._pause_backend at 0x7fc1093e6170> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/backcall/backcall.py:104\u001b[0m, in \u001b[0;36mcallback_prototype.<locals>.adapt.<locals>.adapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 kwargs\u001b[38;5;241m.\u001b[39mpop(name)\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/wandb_init.py:379\u001b[0m, in \u001b[0;36m_WandbInit._pause_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    377\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mlog_code(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    378\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved code: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, res)\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpublish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface.py:609\u001b[0m, in \u001b[0;36mInterfaceBase.publish_pause\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpublish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     pause \u001b[38;5;241m=\u001b[39m pb\u001b[38;5;241m.\u001b[39mPauseRequest()\n\u001b[0;32m--> 609\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish_pause\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpause\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_shared.py:279\u001b[0m, in \u001b[0;36mInterfaceShared._publish_pause\u001b[0;34m(self, pause)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish_pause\u001b[39m(\u001b[38;5;28mself\u001b[39m, pause: pb\u001b[38;5;241m.\u001b[39mPauseRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(pause\u001b[38;5;241m=\u001b[39mpause)\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mindeye/lib/python3.10/site-packages/wandb/sdk/interface/interface_queue.py:49\u001b[0m, in \u001b[0;36mInterfaceQueue._publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_publish\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m, local: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_check \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m---> 49\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m local:\n\u001b[1;32m     51\u001b[0m         record\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mlocal \u001b[38;5;241m=\u001b[39m local\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "def get_img_tensor(data, index_arr, batch_size):\n",
    "    # Assuming the shape of index_arr is [num_iter_per_batch, batch_size * number_of_subj, 16]\n",
    "    num_iter_per_batch = index_arr.shape[0]\n",
    "    number_of_subj = index_arr.shape[1] // batch_size\n",
    "    iter_batches = []\n",
    "\n",
    "    for iter_idx in tqdm(range(num_iter_per_batch)):\n",
    "        batch_emb_arr = []\n",
    "\n",
    "        # Iterate over each index in the batch for this iteration\n",
    "        for subj_idx in range(batch_size * number_of_subj):\n",
    "            ind = index_arr[iter_idx, subj_idx]\n",
    "            sing_arr = []\n",
    "            for i in ind:\n",
    "                path_emb = data[i]\n",
    "                emb = torch.load(path_emb, map_location='cpu').squeeze(0)\n",
    "                sing_arr.append(emb)\n",
    "            sing_ten = torch.stack(sing_arr)\n",
    "            batch_emb_arr.append(sing_ten)\n",
    "\n",
    "        # Stack all embeddings for this iteration into a single tensor\n",
    "        iter_batch_tensor = torch.stack(batch_emb_arr)\n",
    "        iter_batches.append(iter_batch_tensor)\n",
    "\n",
    "    # Stack all iteration tensors into the final 3D tensor\n",
    "    emb_tensor_3d = torch.stack(iter_batches)\n",
    "    return emb_tensor_3d\n",
    "\n",
    "\n",
    "x = get_img_tensor(imgemb_dataset, image_iters, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc70eaf-a52a-47e1-ab70-1cfb30fcdba5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
