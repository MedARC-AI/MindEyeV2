{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3e35fc-9f53-4395-963c-f701c7abe42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/admin/home-jonxu/mambaforge/envs/fmri/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-08 18:32:25,157] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import webdataset as wds\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# tf32 data type is faster than standard float32s\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "global_batch_size = batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 1268644\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "\n",
    "# set data_type to match your mixed precision\n",
    "if accelerator.mixed_precision == \"bf16\":\n",
    "    data_type = torch.bfloat16\n",
    "elif accelerator.mixed_precision == \"fp16\":\n",
    "    data_type = torch.float16\n",
    "else:\n",
    "    data_type = torch.float32\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: behav_opt2_512dim_30epoch\n",
      "['--data_path=/weka/proj-fmri/shared/mindeyev2_dataset', '--model_name=behav_opt2_512dim_30epoch', '--subj=1', '--batch_size=128', '--no-blurry_recon', '--hidden_dim=1024', '--num_sessions=3', '--seq_len=1', '--max_lr=3e-4', '--mixup_pct=.66', '--num_epochs=30', '--no-use_image_aug', '--ckpt_interval=999', '--no-ckpt_saving', '--wandb_log']\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"behav_opt2_512dim_30epoch\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the above cells\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f\"--data_path=/weka/proj-fmri/shared/mindeyev2_dataset \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --subj=1 --batch_size={batch_size} --no-blurry_recon --hidden_dim=1024 --num_sessions=3 --seq_len=1 \\\n",
    "                    --max_lr=3e-4 --mixup_pct=.66 --num_epochs=30 --no-use_image_aug --ckpt_interval=999 --no-ckpt_saving \\\n",
    "                    --wandb_log\"\n",
    "    \n",
    "    jupyter_args = jupyter_args.split()\n",
    "    print(jupyter_args)\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=\"/fsx/proj-fmri/shared/natural-scenes-dataset\",\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,5,7],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_sessions\", type=int, default=0,\n",
    "    help=\"Number of training sessions to include (zero = all sessions)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=32,\n",
    "    help=\"Batch size can be increased by 10x if only training v2c and not diffusion diffuser\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--resume_from_ckpt\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"if not using wandb and want to resume from a ckpt\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=100.,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=120,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seq_len\",type=int,default=1,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "\n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), (0.6,1), p=0.3),\n",
    "        kornia.augmentation.Resize((224, 224)),\n",
    "        kornia.augmentation.RandomHorizontalFlip(p=0.3),\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.3),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c023f24-5233-4a15-a2f5-78487b3a8546",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "995d9983-463c-4d8b-b96d-742799e2c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_sessions = 3 batch_size = 128 num_iterations_per_epoch = 100 num_samples_per_epoch = 12800\n"
     ]
    }
   ],
   "source": [
    "def my_split_by_node(urls): return urls\n",
    "\n",
    "nsessions_allsubj=np.array([40, 40, 32, 30, 40, 32, 40, 30])\n",
    "\n",
    "if num_sessions == 0: num_sessions = nsessions_allsubj[subj-1]\n",
    "\n",
    "num_samples_per_epoch = 12800 // num_devices \n",
    "# or if you want less samples per epoch if you're using less data, can do (750*num_sessions) // num_devices \n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // batch_size\n",
    "\n",
    "print(\"num_sessions =\", num_sessions, \"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar\n",
      "/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar\n"
     ]
    }
   ],
   "source": [
    "train_url = f\"{data_path}/wds/subj0{subj}/train/\" + \"{0..\" + f\"{num_sessions-1}\" + \"}.tar\"\n",
    "print(train_url)\n",
    "\n",
    "train_data = wds.WebDataset(train_url,resampled=True,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\\\n",
    "                    .with_epoch(num_samples_per_epoch)\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "\n",
    "# if using same test dataset as MindEyeV1:\n",
    "test_url = f\"{data_path}/wds/subj0{subj}/test/\" + \"0.tar\"\n",
    "\n",
    "## if using all shared1000 test images now that they released the last 3 sessions per participant:\n",
    "# test_url = f\"{data_path}/wds/subj0{subj}/new_test/\" + \"0.tar\"\n",
    "\n",
    "print(test_url)\n",
    "\n",
    "num_test = 2770 # total number of test samples\n",
    "test_batch_size = 300 # number of test samples to use during validation\n",
    "test_data = wds.WebDataset(test_url,resampled=False,nodesplitter=my_split_by_node)\\\n",
    "                    .shuffle(750, initial=1500, rng=random.Random(42))\\\n",
    "                    .decode(\"torch\")\\\n",
    "                    .rename(behav=\"behav.npy\", past_behav=\"past_behav.npy\", future_behav=\"future_behav.npy\", olds_behav=\"olds_behav.npy\")\\\n",
    "                    .to_tuple(*[\"behav\", \"past_behav\", \"future_behav\", \"olds_behav\"])\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size=num_test, shuffle=False, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "159ec41c-8767-4efb-9fe2-a5cf736e6bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example = next(iter(train_dl))\n",
    "# behaviour = example[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20567d10-0f0f-4c90-80dd-1a1300768649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colNames = [\n",
    "# \"cocoidx\", #0\n",
    "# \"subject\", #1\n",
    "# \"session\", #2\n",
    "# \"run\", #3\n",
    "# \"trial\", #4\n",
    "# \"global_trial\", #5\n",
    "# \"time\", #6\n",
    "# \"isold\", #7\n",
    "# \"iscorrect\", #8\n",
    "# \"rt\", # 0 = no RT #9\n",
    "# \"changemind\", #10\n",
    "# \"isoldcurrent\", #11\n",
    "# \"iscorrectcurrent\", #12\n",
    "# \"total1\", #13\n",
    "# \"total2\", #14\n",
    "# \"button\", #15\n",
    "# \"shared1000\" #16\n",
    "# ]\n",
    "\n",
    "# batch = example[0]\n",
    "\n",
    "# # Iterate through each feature and plot its distribution\n",
    "# num_features = batch.shape[-1]\n",
    "# for i in range(num_features):\n",
    "#     feature_values = batch[:, :, i].flatten()  # Extract values for the current feature\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "#     plt.hist(feature_values, bins=20)  # Adjust the number of bins as needed\n",
    "#     plt.title(f\"{colNames[i]}\")\n",
    "#     plt.xlabel(\"Value\")\n",
    "#     plt.ylabel(\"Frequency\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203b060a-2dd2-4c35-929b-c576be82eb52",
   "metadata": {},
   "source": [
    "### check dataloaders are working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e7a9c68c-c3c9-4080-bd99-067c4486dc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_vox_indices = []\n",
    "# test_73k_images = []\n",
    "# for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):\n",
    "#     test_vox_indices = np.append(test_vox_indices, behav[:,0,5].cpu().numpy())\n",
    "#     test_73k_images = np.append(test_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# test_vox_indices = test_vox_indices.astype(np.int16)\n",
    "# print(test_i, (test_i+1) * num_test, len(test_vox_indices))\n",
    "# print(\"---\\n\")\n",
    "\n",
    "# train_vox_indices = []\n",
    "# train_73k_images = []\n",
    "# for train_i, (behav, past_behav, future_behav, old_behav) in enumerate(train_dl):\n",
    "#     train_vox_indices = np.append(train_vox_indices, behav[:,0,5].long().cpu().numpy())\n",
    "#     train_73k_images = np.append(train_73k_images, behav[:,0,0].cpu().numpy())\n",
    "# train_vox_indices = train_vox_indices.astype(np.int16)\n",
    "# print(train_i, (train_i+1) * batch_size, len(train_vox_indices))\n",
    "\n",
    "# ## example output ##\n",
    "# # 0 2770 2770\n",
    "# # ---\n",
    "# # 99 12800 12800"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fad12c-f9fb-4408-8fd4-9bca324ad634",
   "metadata": {},
   "source": [
    "## Load data and images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "039dd330-7339-4f88-8f00-45f95e47baa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj01 betas loaded into memory\n",
      "voxels torch.Size([30000, 15724])\n",
      "images torch.Size([73000, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# load betas (use fp32 hdf5 file to have more flexible precision and include all sessions inc. those released after algonauts)\n",
    "f = h5py.File(f'{data_path}/betas_all_subj0{subj}_fp32.hdf5', 'r')\n",
    "\n",
    "voxels = f['betas'][:]\n",
    "print(f\"subj0{subj} betas loaded into memory\")\n",
    "voxels = torch.Tensor(voxels).to(\"cpu\").to(data_type)\n",
    "print(\"voxels\", voxels.shape)\n",
    "num_voxels = voxels.shape[-1]\n",
    "\n",
    "# load orig images\n",
    "f = h5py.File(f'{data_path}/coco_images_224_float16.hdf5', 'r')\n",
    "images = f['images'][:]\n",
    "images = torch.Tensor(images).to(\"cpu\").to(data_type)\n",
    "print(\"images\", images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT-L/14 cuda:0\n"
     ]
    }
   ],
   "source": [
    "from models import Clipper\n",
    "clip_model = Clipper(\"ViT-L/14\", device=torch.device(f\"cuda:{local_rank}\"), hidden_state=True, norm_embs=True)\n",
    "clip_seq_dim = 257\n",
    "clip_emb_dim = 768 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3bdfbf9-ceaf-407a-9fff-f8a2b7dad7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavior embeddings\n",
    "behav_embed_dim = 512\n",
    "target_behaviours = [7,8,10,11,12]\n",
    "# target_behaviours = list(range(1,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL\n",
    "    autoenc = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=data_type, cache_dir=\"/weka/proj-fmri/shared/cache\")\n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c8eee-9834-437d-bb60-b38faef50138",
   "metadata": {},
   "source": [
    "#### downsampled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d1ba8dd-64c2-4ac9-947e-725b7f2e3e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "if blurry_recon:\n",
    "    if utils.is_interactive(): display(utils.torch_to_Image(images[[30]]))\n",
    "\n",
    "    input_batch = images[[30]].to(device)\n",
    "    print(input_batch.shape)\n",
    "\n",
    "    downsampled_image = nn.functional.interpolate(input_batch, size=(8, 8), mode='bilinear', align_corners=False)\n",
    "    re_upsampled_image = nn.functional.interpolate(downsampled_image, size=(128, 128), mode='nearest')\n",
    "    re_upsampled_enc = autoenc.encode(2*re_upsampled_image-1).latent_dist.mode() * 0.18215\n",
    "    print(re_upsampled_enc.shape)\n",
    "    \n",
    "    if utils.is_interactive(): display(utils.torch_to_Image((autoenc.decode(re_upsampled_enc/0.18215).sample / 2 + 0.5).clamp(0,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "torch.Size([2, 15724])\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer\n",
    "    def __init__(self, input_size, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linear = torch.nn.Linear(input_size, out_features)\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3: # do ridge mapping the same way for each timepoint\n",
    "            ridge_out = torch.cat([self.linear(x[:,xi])[:,None] for xi in range(x.shape[1])], dim=1)\n",
    "            return ridge_out\n",
    "        else:\n",
    "            x = self.linear(x).unsqueeze(1)\n",
    "            return x\n",
    "        \n",
    "model.ridge = RidgeRegression(voxels.shape[1], out_features=hidden_dim)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "\n",
    "fake_input = torch.randn((2,voxels.shape[1]))\n",
    "print(fake_input.shape)\n",
    "print(model.ridge(fake_input).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12f62990-3aa6-42f5-aa26-56bf24bf931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734dc1ea-d193-4a00-b669-7c6580663481",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavs = example[0][:,:,target_behaviours]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "79ff7521-8b3c-4ed9-8fb2-a489bc45b50a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must pass 2-d input. shape=(128, 1, 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbehavs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/fmri/lib/python3.10/site-packages/pandas/core/frame.py:785\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    774\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    775\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[1;32m    776\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    782\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[1;32m    783\u001b[0m         )\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 785\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[0;32m~/mambaforge/envs/fmri/lib/python3.10/site-packages/pandas/core/internals/construction.py:314\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    308\u001b[0m     _copy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    309\u001b[0m         copy_on_sanitize\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m astype_is_view(values\u001b[38;5;241m.\u001b[39mdtype, dtype))\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    313\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values, copy\u001b[38;5;241m=\u001b[39m_copy)\n\u001b[0;32m--> 314\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     values \u001b[38;5;241m=\u001b[39m _prep_ndarraylike(values, copy\u001b[38;5;241m=\u001b[39mcopy_on_sanitize)\n",
      "File \u001b[0;32m~/mambaforge/envs/fmri/lib/python3.10/site-packages/pandas/core/internals/construction.py:592\u001b[0m, in \u001b[0;36m_ensure_2d\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    590\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape((values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass 2-d input. shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: Must pass 2-d input. shape=(128, 1, 15)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(behavs.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3602c333-d029-465c-8fb4-c3ccffdba6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "723,354,136 total\n",
      "723,354,136 trainable\n",
      "param counts:\n",
      "739,456,536 total\n",
      "739,456,536 trainable\n",
      "fake_input.shape torch.Size([2, 1, 1024])\n",
      "fake_behaviour.shape torch.Size([2, 5])\n",
      "torch.Size([2, 257, 768]) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from diffusers.models.vae import Decoder\n",
    "class BrainNetwork(nn.Module):\n",
    "    def __init__(self, out_dim=768, in_dim=15724, seq_len=2, h=4096, n_blocks=4, drop=.15, clip_size=768, behav_size=0, behav_embed=2):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.h = h\n",
    "        self.clip_size = clip_size\n",
    "        self.behav_embd = behav_embed\n",
    "        \n",
    "        # Mixer Blocks\n",
    "        self.mixer_blocks1 = nn.ModuleList([\n",
    "            self.mixer_block1(h, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.mixer_blocks2 = nn.ModuleList([\n",
    "            self.mixer_block2(seq_len, drop) for _ in range(n_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Output linear layer\n",
    "        behav_tensor_size = behav_size * behav_embed\n",
    "        self.clin1 = nn.Linear(h * seq_len + behav_tensor_size, out_dim, bias=True) # 1024 + behav_tensor_size, 197376\n",
    "        # self.clin1 = nn.Linear(h * seq_len, out_dim, bias=True) # 1024 + behav_size, 197376\n",
    "        \n",
    "        # Behaviour embedding layer\n",
    "        self.behav_embedder = nn.Embedding(3, behav_embed)\n",
    "        # self.behav_project = nn.Linear(behav_tensor_size, clip_size)\n",
    "\n",
    "        self.clip_proj = nn.Sequential(\n",
    "            nn.LayerNorm(clip_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(clip_size, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, 2048),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, clip_size)\n",
    "        )\n",
    "\n",
    "        if blurry_recon:\n",
    "            self.blin1 = nn.Sequential(\n",
    "                nn.Linear(h*seq_len, 4096, bias=True),\n",
    "                nn.LayerNorm(4096),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(4096, 4096))\n",
    "            self.bgroupnorm = nn.GroupNorm(1, 256)\n",
    "            self.bupsampler = Decoder(\n",
    "                in_channels=256,\n",
    "                out_channels=4,\n",
    "                up_block_types=[\"UpDecoderBlock2D\",\"UpDecoderBlock2D\",\"UpDecoderBlock2D\"],\n",
    "                block_out_channels=[32, 64, 128],\n",
    "                layers_per_block=1,\n",
    "            )\n",
    "        \n",
    "    def mixer_block1(self, h, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(h),\n",
    "            self.mlp(h, h, drop),  # Token mixing\n",
    "        )\n",
    "\n",
    "    def mixer_block2(self, seq_len, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.LayerNorm(seq_len),\n",
    "            self.mlp(seq_len, seq_len, drop)  # Channel mixing\n",
    "        )\n",
    "    \n",
    "    def mlp(self, in_dim, out_dim, drop):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(out_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, behaviour=None, b=torch.Tensor([0.])):        \n",
    "        # Option 1: Append behaviour here (resulting in [batch, 1, 1024 + behaviours])\n",
    "        # will need to change all the mixer blocks, messy. also would get drowned out by rest of fmri data\n",
    "        if behaviour is not None:\n",
    "            behaviour = behaviour.to(torch.int) + 1 # Adding 1 to accomodate -1 values\n",
    "        \n",
    "        # Mixer blocks\n",
    "        residual1 = x # batch, 1, 1024\n",
    "        residual2 = x.permute(0,2,1)\n",
    "        for block1, block2 in zip(self.mixer_blocks1,self.mixer_blocks2):\n",
    "            x = block1(x) + residual1\n",
    "            residual1 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "            x = block2(x) + residual2\n",
    "            residual2 = x\n",
    "            x = x.permute(0,2,1)\n",
    "            \n",
    "        # batch, 1, 1024\n",
    "        x = x.reshape(x.size(0), -1) # batch, 1024\n",
    "        # Option 2: Append behaviour here (resulting in [batch, 1024 + behaviours])\n",
    "        if behaviour is not None:\n",
    "            behaviour = self.behav_embedder(behaviour)\n",
    "            behaviour = behaviour.view(behaviour.size(0), -1)\n",
    "            x = torch.cat((x, behaviour), dim=1)\n",
    "        c = self.clin1(x) # batch, 197376\n",
    "        c = c.reshape(len(c), -1, self.clip_size) # batch, 257, 768\n",
    "        \n",
    "        # Option 3: Append behaviour here (resulting in [batch, 258, 768])\n",
    "        # Not a good idea. c will be a different seq length than our target\n",
    "        # if behaviour is not None:\n",
    "        #     embedding = self.behav_embedder(behaviour)\n",
    "        #     embedding = embedding.view(embedding.size(0), -1)\n",
    "        #     proj = self.behav_project(embedding)\n",
    "        #     proj = proj.unsqueeze(1)\n",
    "        #     c = torch.cat((c, proj), dim = 1)\n",
    "\n",
    "        c = self.clip_proj(c) # batch, 257, 768\n",
    "\n",
    "        if blurry_recon:\n",
    "            b = self.blin1(x)\n",
    "            b = b.reshape(len(b), 256, 4, 4)\n",
    "            b = self.bgroupnorm(b)\n",
    "            b = self.bupsampler(b)\n",
    "        \n",
    "        return c, b\n",
    "\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=seq_len, clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, behav_size=len(target_behaviours), behav_embed = behav_embed_dim) \n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "fake_input = torch.randn((2,seq_len,hidden_dim))\n",
    "fake_behaviour = torch.randint(-1, 2, (2, len(target_behaviours)))\n",
    "print(\"fake_input.shape\", fake_input.shape)\n",
    "print(\"fake_behaviour.shape\", fake_behaviour.shape)\n",
    "clip_, blur_ = model.backbone(fake_input, fake_behaviour)\n",
    "# clip_, blur_ = model.backbone(fake_input)\n",
    "\n",
    "print(clip_.shape, blur_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 3000\n",
      "param counts:\n",
      "739,456,536 total\n",
      "739,456,536 trainable\n",
      "\n",
      "Done with model preparations!\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):    \n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    print(f'saving {ckpt_path}',flush=True)\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    try:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    except:\n",
    "        print(\"Couldn't save... moving on to prevent crashing.\")\n",
    "    del unwrapped_model\n",
    "\n",
    "utils.count_params(model)\n",
    "print(\"\\nDone with model preparations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "## Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb mindeyev2 run behav_opt2_512dim_30epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjonxuxu\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb_config:\n",
      " {'model_name': 'behav_opt2_512dim_30epoch', 'global_batch_size': 128, 'batch_size': 128, 'num_epochs': 30, 'clip_scale': 1.0, 'blur_scale': 100.0, 'use_image_aug': False, 'max_lr': 0.0003, 'mixup_pct': 0.66, 'ckpt_interval': 999, 'ckpt_saving': False, 'seed': 42, 'distributed': False, 'num_devices': 1, 'world_size': 1, 'train_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/train/{0..2}.tar', 'test_url': '/weka/proj-fmri/shared/mindeyev2_dataset/wds/subj01/test/0.tar', 'behav_embed': 512}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/weka/proj-fmri/jonxu/MindEyeV2/src/wandb/run-20240108_183448-msu8b68o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jonxuxu/mindeyev2/runs/msu8b68o' target=\"_blank\">behav_opt2_512dim_30epoch</a></strong> to <a href='https://wandb.ai/jonxuxu/mindeyev2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jonxuxu/mindeyev2' target=\"_blank\">https://wandb.ai/jonxuxu/mindeyev2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jonxuxu/mindeyev2/runs/msu8b68o' target=\"_blank\">https://wandb.ai/jonxuxu/mindeyev2/runs/msu8b68o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeyev2'\n",
    "    wandb_run = model_name\n",
    "    wandb_notes = ''\n",
    "    \n",
    "    print(f\"wandb {wandb_project} run {wandb_run}\")\n",
    "    wandb.login()\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"train_url\": train_url,\n",
    "      \"test_url\": test_url,\n",
    "        \"behav_embed\": behav_embed_dim\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    if False: # wandb_auto_resume\n",
    "        print(\"wandb_id:\",model_name)\n",
    "        wandb.init(\n",
    "            id = model_name,\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            resume=\"allow\",\n",
    "        )\n",
    "    else:\n",
    "        wandb.init(\n",
    "            project=wandb_project,\n",
    "            name=wandb_run,\n",
    "            config=wandb_config,\n",
    "            notes=wandb_notes,\n",
    "            settings=wandb.Settings(start_method=\"fork\")\n",
    "        )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "# Optionally resume from checkpoint #\n",
    "if resume_from_ckpt:\n",
    "    print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(\"Epoch\",epoch)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    del checkpoint\n",
    "elif wandb_log:\n",
    "    if wandb.run.resumed:\n",
    "        print(\"\\n---resuming from last.pth ckpt---\\n\")\n",
    "        checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "        epoch = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        del checkpoint\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, train_dl, lr_scheduler = accelerator.prepare(\n",
    "model, optimizer, train_dl, lr_scheduler\n",
    ")\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behav_opt2_512dim_30epoch starting with epoch 0 / 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 0/30 [00:00<?, ?it/s]/admin/home-jonxu/mambaforge/envs/fmri/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/30 [01:18<?, ?it/s, test/blurry_pixcorr=0, test/loss=4.16, test/loss_blurry_total=0, test/loss_clip_total=4.16, test/num_steps=1, test/test_bwd_pct_correct=0.0667, test/test_fwd_pct_correct=0.193, train/blurry_pixcorr=0, train/bwd_pct_correct=0.168, train/fwd_pct_correct=0.366, train/loss=3.57, train/loss_blurry_total=0, train/loss_clip_total=3.57, train/lr=0.000153, train/num_steps=100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 1/30 [02:13<38:10, 78.99s/it, test/blurry_pixcorr=0, test/loss=3.1, test/loss_blurry_total=0, test/loss_clip_total=3.1, test/num_steps=2, test/test_bwd_pct_correct=0.167, test/test_fwd_pct_correct=0.41, train/blurry_pixcorr=0, train/bwd_pct_correct=0.558, train/fwd_pct_correct=0.619, train/loss=1.02, train/loss_blurry_total=0, train/loss_clip_total=1.02, train/lr=0.0003, train/num_steps=200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 2/30 [03:07<30:09, 64.64s/it, test/blurry_pixcorr=0, test/loss=2.75, test/loss_blurry_total=0, test/loss_clip_total=2.75, test/num_steps=3, test/test_bwd_pct_correct=0.333, test/test_fwd_pct_correct=0.363, train/blurry_pixcorr=0, train/bwd_pct_correct=0.7, train/fwd_pct_correct=0.673, train/loss=0.563, train/loss_blurry_total=0, train/loss_clip_total=0.563, train/lr=0.000299, train/num_steps=300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | 3/30 [04:02<26:59, 59.99s/it, test/blurry_pixcorr=0, test/loss=2.86, test/loss_blurry_total=0, test/loss_clip_total=2.86, test/num_steps=4, test/test_bwd_pct_correct=0.343, test/test_fwd_pct_correct=0.367, train/blurry_pixcorr=0, train/bwd_pct_correct=0.716, train/fwd_pct_correct=0.673, train/loss=0.479, train/loss_blurry_total=0, train/loss_clip_total=0.479, train/lr=0.000296, train/num_steps=400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 4/30 [04:56<25:04, 57.86s/it, test/blurry_pixcorr=0, test/loss=2.97, test/loss_blurry_total=0, test/loss_clip_total=2.97, test/num_steps=5, test/test_bwd_pct_correct=0.317, test/test_fwd_pct_correct=0.383, train/blurry_pixcorr=0, train/bwd_pct_correct=0.708, train/fwd_pct_correct=0.663, train/loss=0.465, train/loss_blurry_total=0, train/loss_clip_total=0.465, train/lr=0.000292, train/num_steps=500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 5/30 [05:51<23:33, 56.53s/it, test/blurry_pixcorr=0, test/loss=3.07, test/loss_blurry_total=0, test/loss_clip_total=3.07, test/num_steps=6, test/test_bwd_pct_correct=0.313, test/test_fwd_pct_correct=0.343, train/blurry_pixcorr=0, train/bwd_pct_correct=0.704, train/fwd_pct_correct=0.659, train/loss=0.467, train/loss_blurry_total=0, train/loss_clip_total=0.467, train/lr=0.000285, train/num_steps=600]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 6/30 [06:45<22:20, 55.84s/it, test/blurry_pixcorr=0, test/loss=3.03, test/loss_blurry_total=0, test/loss_clip_total=3.03, test/num_steps=7, test/test_bwd_pct_correct=0.347, test/test_fwd_pct_correct=0.317, train/blurry_pixcorr=0, train/bwd_pct_correct=0.705, train/fwd_pct_correct=0.662, train/loss=0.461, train/loss_blurry_total=0, train/loss_clip_total=0.461, train/lr=0.000277, train/num_steps=700]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 7/30 [07:40<21:13, 55.38s/it, test/blurry_pixcorr=0, test/loss=3.15, test/loss_blurry_total=0, test/loss_clip_total=3.15, test/num_steps=8, test/test_bwd_pct_correct=0.36, test/test_fwd_pct_correct=0.337, train/blurry_pixcorr=0, train/bwd_pct_correct=0.712, train/fwd_pct_correct=0.662, train/loss=0.456, train/loss_blurry_total=0, train/loss_clip_total=0.456, train/lr=0.000267, train/num_steps=800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 8/30 [08:34<20:13, 55.15s/it, test/blurry_pixcorr=0, test/loss=3.01, test/loss_blurry_total=0, test/loss_clip_total=3.01, test/num_steps=9, test/test_bwd_pct_correct=0.39, test/test_fwd_pct_correct=0.327, train/blurry_pixcorr=0, train/bwd_pct_correct=0.712, train/fwd_pct_correct=0.664, train/loss=0.441, train/loss_blurry_total=0, train/loss_clip_total=0.441, train/lr=0.000256, train/num_steps=900]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 9/30 [09:28<19:12, 54.88s/it, test/blurry_pixcorr=0, test/loss=3.19, test/loss_blurry_total=0, test/loss_clip_total=3.19, test/num_steps=10, test/test_bwd_pct_correct=0.347, test/test_fwd_pct_correct=0.347, train/blurry_pixcorr=0, train/bwd_pct_correct=0.711, train/fwd_pct_correct=0.661, train/loss=0.436, train/loss_blurry_total=0, train/loss_clip_total=0.436, train/lr=0.000244, train/num_steps=1000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | 10/30 [10:23<18:14, 54.70s/it, test/blurry_pixcorr=0, test/loss=3.08, test/loss_blurry_total=0, test/loss_clip_total=3.08, test/num_steps=11, test/test_bwd_pct_correct=0.377, test/test_fwd_pct_correct=0.333, train/blurry_pixcorr=0, train/bwd_pct_correct=0.715, train/fwd_pct_correct=0.664, train/loss=0.429, train/loss_blurry_total=0, train/loss_clip_total=0.429, train/lr=0.00023, train/num_steps=1100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 11/30 [11:19<17:21, 54.82s/it, test/blurry_pixcorr=0, test/loss=3.14, test/loss_blurry_total=0, test/loss_clip_total=3.14, test/num_steps=12, test/test_bwd_pct_correct=0.417, test/test_fwd_pct_correct=0.36, train/blurry_pixcorr=0, train/bwd_pct_correct=0.707, train/fwd_pct_correct=0.659, train/loss=0.438, train/loss_blurry_total=0, train/loss_clip_total=0.438, train/lr=0.000215, train/num_steps=1200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | 12/30 [12:13<16:28, 54.94s/it, test/blurry_pixcorr=0, test/loss=3.26, test/loss_blurry_total=0, test/loss_clip_total=3.26, test/num_steps=13, test/test_bwd_pct_correct=0.38, test/test_fwd_pct_correct=0.313, train/blurry_pixcorr=0, train/bwd_pct_correct=0.714, train/fwd_pct_correct=0.67, train/loss=0.426, train/loss_blurry_total=0, train/loss_clip_total=0.426, train/lr=0.0002, train/num_steps=1300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                | 13/30 [13:07<15:31, 54.77s/it, test/blurry_pixcorr=0, test/loss=3.25, test/loss_blurry_total=0, test/loss_clip_total=3.25, test/num_steps=14, test/test_bwd_pct_correct=0.39, test/test_fwd_pct_correct=0.343, train/blurry_pixcorr=0, train/bwd_pct_correct=0.707, train/fwd_pct_correct=0.66, train/loss=0.424, train/loss_blurry_total=0, train/loss_clip_total=0.424, train/lr=0.000184, train/num_steps=1400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|                                                                                                                                                                                                                                                                                                                                                                                                                                     | 14/30 [14:01<14:32, 54.54s/it, test/blurry_pixcorr=0, test/loss=3.32, test/loss_blurry_total=0, test/loss_clip_total=3.32, test/num_steps=15, test/test_bwd_pct_correct=0.4, test/test_fwd_pct_correct=0.287, train/blurry_pixcorr=0, train/bwd_pct_correct=0.723, train/fwd_pct_correct=0.668, train/loss=0.403, train/loss_blurry_total=0, train/loss_clip_total=0.403, train/lr=0.000167, train/num_steps=1500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|                                                                                                                                                                                                                                                                                                                                                                                                           | 15/30 [14:55<13:36, 54.40s/it, test/blurry_pixcorr=0, test/loss=3.37, test/loss_blurry_total=0, test/loss_clip_total=3.37, test/num_steps=16, test/test_bwd_pct_correct=0.393, test/test_fwd_pct_correct=0.303, train/blurry_pixcorr=0, train/bwd_pct_correct=0.713, train/fwd_pct_correct=0.659, train/loss=0.409, train/loss_blurry_total=0, train/loss_clip_total=0.409, train/lr=0.00015, train/num_steps=1600]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|                                                                                                                                                                                                                                                                                                                                                                                | 16/30 [15:49<12:40, 54.30s/it, test/blurry_pixcorr=0, test/loss=3.39, test/loss_blurry_total=0, test/loss_clip_total=3.39, test/num_steps=17, test/test_bwd_pct_correct=0.36, test/test_fwd_pct_correct=0.357, train/blurry_pixcorr=0, train/bwd_pct_correct=0.713, train/fwd_pct_correct=0.659, train/loss=0.414, train/loss_blurry_total=0, train/loss_clip_total=0.414, train/lr=0.000133, train/num_steps=1700]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|                                                                                                                                                                                                                                                                                                                                                      | 17/30 [16:44<11:45, 54.30s/it, test/blurry_pixcorr=0, test/loss=3.46, test/loss_blurry_total=0, test/loss_clip_total=3.46, test/num_steps=18, test/test_bwd_pct_correct=0.38, test/test_fwd_pct_correct=0.263, train/blurry_pixcorr=0, train/bwd_pct_correct=0.714, train/fwd_pct_correct=0.664, train/loss=0.403, train/loss_blurry_total=0, train/loss_clip_total=0.403, train/lr=0.000117, train/num_steps=1800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|                                                                                                                                                                                                                                                                                                                            | 18/30 [17:38<10:52, 54.38s/it, test/blurry_pixcorr=0, test/loss=3.49, test/loss_blurry_total=0, test/loss_clip_total=3.49, test/num_steps=19, test/test_bwd_pct_correct=0.433, test/test_fwd_pct_correct=0.31, train/blurry_pixcorr=0, train/bwd_pct_correct=0.703, train/fwd_pct_correct=0.659, train/loss=0.424, train/loss_blurry_total=0, train/loss_clip_total=0.424, train/lr=0.000101, train/num_steps=1900]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|                                                                                                                                                                                                                                                                                                 | 19/30 [18:33<09:58, 54.43s/it, test/blurry_pixcorr=0, test/loss=3.55, test/loss_blurry_total=0, test/loss_clip_total=3.55, test/num_steps=20, test/test_bwd_pct_correct=0.407, test/test_fwd_pct_correct=0.53, train/blurry_pixcorr=0, train/bwd_pct_correct=0.947, train/fwd_pct_correct=0.948, train/loss=0.0869, train/loss_blurry_total=0, train/loss_clip_total=0.0869, train/lr=8.51e-5, train/num_steps=2000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|                                                                                                                                                                                                                                                                       | 20/30 [19:28<09:04, 54.49s/it, test/blurry_pixcorr=0, test/loss=3.39, test/loss_blurry_total=0, test/loss_clip_total=3.39, test/num_steps=21, test/test_bwd_pct_correct=0.43, test/test_fwd_pct_correct=0.593, train/blurry_pixcorr=0, train/bwd_pct_correct=0.952, train/fwd_pct_correct=0.952, train/loss=0.0788, train/loss_blurry_total=0, train/loss_clip_total=0.0788, train/lr=7.05e-5, train/num_steps=2100]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|                                                                                                                                                                                                                                            | 21/30 [20:22<08:10, 54.50s/it, test/blurry_pixcorr=0, test/loss=3.17, test/loss_blurry_total=0, test/loss_clip_total=3.17, test/num_steps=22, test/test_bwd_pct_correct=0.457, test/test_fwd_pct_correct=0.627, train/blurry_pixcorr=0, train/bwd_pct_correct=0.946, train/fwd_pct_correct=0.946, train/loss=0.0852, train/loss_blurry_total=0, train/loss_clip_total=0.0852, train/lr=5.67e-5, train/num_steps=2200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|                                                                                                                                                                                                                  | 22/30 [21:17<07:16, 54.58s/it, test/blurry_pixcorr=0, test/loss=2.97, test/loss_blurry_total=0, test/loss_clip_total=2.97, test/num_steps=23, test/test_bwd_pct_correct=0.473, test/test_fwd_pct_correct=0.617, train/blurry_pixcorr=0, train/bwd_pct_correct=0.952, train/fwd_pct_correct=0.952, train/loss=0.0753, train/loss_blurry_total=0, train/loss_clip_total=0.0753, train/lr=4.42e-5, train/num_steps=2300]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|                                                                                                                                                                                       | 23/30 [22:12<06:22, 54.63s/it, test/blurry_pixcorr=0, test/loss=2.76, test/loss_blurry_total=0, test/loss_clip_total=2.76, test/num_steps=24, test/test_bwd_pct_correct=0.503, test/test_fwd_pct_correct=0.597, train/blurry_pixcorr=0, train/bwd_pct_correct=0.947, train/fwd_pct_correct=0.947, train/loss=0.0792, train/loss_blurry_total=0, train/loss_clip_total=0.0792, train/lr=3.29e-5, train/num_steps=2400]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|                                                                                                                                                              | 24/30 [23:06<05:28, 54.74s/it, test/blurry_pixcorr=0, test/loss=2.6, test/loss_blurry_total=0, test/loss_clip_total=2.6, test/num_steps=25, test/test_bwd_pct_correct=0.523, test/test_fwd_pct_correct=0.597, train/blurry_pixcorr=0, train/bwd_pct_correct=0.95, train/fwd_pct_correct=0.95, train/loss=0.0741, train/loss_blurry_total=0, train/loss_clip_total=0.0741, train/lr=2.32e-5, train/num_steps=2500]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|                                                                                                                                   | 25/30 [24:01<04:33, 54.61s/it, test/blurry_pixcorr=0, test/loss=2.48, test/loss_blurry_total=0, test/loss_clip_total=2.48, test/num_steps=26, test/test_bwd_pct_correct=0.533, test/test_fwd_pct_correct=0.58, train/blurry_pixcorr=0, train/bwd_pct_correct=0.942, train/fwd_pct_correct=0.943, train/loss=0.0851, train/loss_blurry_total=0, train/loss_clip_total=0.0851, train/lr=1.5e-5, train/num_steps=2600]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|                                                                                                         | 26/30 [24:54<03:37, 54.44s/it, test/blurry_pixcorr=0, test/loss=2.4, test/loss_blurry_total=0, test/loss_clip_total=2.4, test/num_steps=27, test/test_bwd_pct_correct=0.523, test/test_fwd_pct_correct=0.587, train/blurry_pixcorr=0, train/bwd_pct_correct=0.947, train/fwd_pct_correct=0.947, train/loss=0.0796, train/loss_blurry_total=0, train/loss_clip_total=0.0796, train/lr=8.54e-6, train/num_steps=2700]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|                                                                               | 27/30 [25:49<02:42, 54.30s/it, test/blurry_pixcorr=0, test/loss=2.36, test/loss_blurry_total=0, test/loss_clip_total=2.36, test/num_steps=28, test/test_bwd_pct_correct=0.52, test/test_fwd_pct_correct=0.583, train/blurry_pixcorr=0, train/bwd_pct_correct=0.944, train/fwd_pct_correct=0.944, train/loss=0.0841, train/loss_blurry_total=0, train/loss_clip_total=0.0841, train/lr=3.85e-6, train/num_steps=2800]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|                                                    | 28/30 [26:43<01:48, 54.22s/it, test/blurry_pixcorr=0, test/loss=2.34, test/loss_blurry_total=0, test/loss_clip_total=2.34, test/num_steps=29, test/test_bwd_pct_correct=0.52, test/test_fwd_pct_correct=0.58, train/blurry_pixcorr=0, train/bwd_pct_correct=0.942, train/fwd_pct_correct=0.943, train/loss=0.0866, train/loss_blurry_total=0, train/loss_clip_total=0.0866, train/lr=9.93e-7, train/num_steps=2900]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|                          | 29/30 [27:37<00:54, 54.18s/it, test/blurry_pixcorr=0, test/loss=2.34, test/loss_blurry_total=0, test/loss_clip_total=2.34, test/num_steps=30, test/test_bwd_pct_correct=0.52, test/test_fwd_pct_correct=0.583, train/blurry_pixcorr=0, train/bwd_pct_correct=0.949, train/fwd_pct_correct=0.949, train/loss=0.0775, train/loss_blurry_total=0, train/loss_clip_total=0.0775, train/lr=1.24e-8, train/num_steps=3000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 30/30 [27:37<00:00, 55.25s/it, test/blurry_pixcorr=0, test/loss=2.34, test/loss_blurry_total=0, test/loss_clip_total=2.34, test/num_steps=30, test/test_bwd_pct_correct=0.52, test/test_fwd_pct_correct=0.583, train/blurry_pixcorr=0, train/bwd_pct_correct=0.949, train/fwd_pct_correct=0.949, train/loss=0.0775, train/loss_blurry_total=0, train/loss_clip_total=0.0775, train/lr=1.24e-8, train/num_steps=3000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===Finished!===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_voxel = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    \n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "\n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    test_loss_blurry_total = 0.\n",
    "\n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "    \n",
    "    for train_i, (behav, past_behav, future_behav, old_behav) in enumerate(train_dl):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            voxel = voxels[behav[:,0,5].cpu().long()].to(device)\n",
    "            image = images[behav[:,0,0].cpu().long()].to(device).float()\n",
    "            action = behav[:, 0, target_behaviours].to(device).half() # batch, len(target_behaviours)\n",
    "\n",
    "            if seq_len > 1:\n",
    "                past_behavior = past_behav[:,:(seq_len-1),5].cpu().long()\n",
    "                past_voxel = voxels[past_behavior]\n",
    "                if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                    past_voxel[torch.where(past_behavior==-1)[0]] = 0\n",
    "                past_voxel = torch.Tensor(past_voxel).to(data_type).to(device)\n",
    "    \n",
    "            if blurry_recon:\n",
    "                # using pixellated blurry ground truth references seems to do better than gaussian blur\n",
    "                rand = np.random.rand() \n",
    "                if rand > .66: # using different interpolation sizes for data augmentation\n",
    "                    blurry_image = utils.resize(nn.functional.interpolate(image, size=(8, 8), mode='bilinear', align_corners=False),128)\n",
    "                elif rand > .33:\n",
    "                    blurry_image = utils.resize(nn.functional.interpolate(image, size=(12, 12), mode='bilinear', align_corners=False),128)\n",
    "                else:\n",
    "                    blurry_image = utils.resize(nn.functional.interpolate(image, size=(24, 24), mode='bilinear', align_corners=False), 128)\n",
    "                # blurry_image = utils.add_saturation(blurry_image) # PS: I'm not sure that this helps\n",
    "                blurry_image_enc = autoenc.encode(2*blurry_image-1).latent_dist.mode() * 0.18215\n",
    "            \n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "    \n",
    "            clip_target = clip_model.embed_image(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "    \n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                voxel, perm, betas, select = utils.mixco(voxel)\n",
    "    \n",
    "            voxel_ridge = model.ridge(voxel)\n",
    "\n",
    "            if seq_len > 1:\n",
    "                past_voxel_ridge = model.ridge(past_voxel)\n",
    "                voxel_ridge = torch.cat((voxel_ridge, past_voxel_ridge), axis=1)\n",
    "            clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge, action)\n",
    "            # clip_voxels, blurry_image_enc_ = model.backbone(voxel_ridge)\n",
    "            \n",
    "            clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "            clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "    \n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                loss_clip = utils.mixco_nce(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006, \n",
    "                    perm=perm, betas=betas, select=select)\n",
    "            else:\n",
    "                epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=epoch_temp)\n",
    "\n",
    "            loss_clip_total += loss_clip.item()\n",
    "            loss_clip *= clip_scale\n",
    "            loss = loss_clip\n",
    "    \n",
    "            if blurry_recon:\n",
    "                loss_blurry = l1(blurry_image_enc_, blurry_image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "                loss_blurry *= blur_scale\n",
    "                loss += loss_blurry\n",
    "    \n",
    "            # forward and backward top 1 accuracy        \n",
    "            labels = torch.arange(len(clip_target_norm)).to(clip_voxels_norm.device) \n",
    "            fwd_percent_correct += utils.topk(torch.abs(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm)), labels, k=1).item()\n",
    "            bwd_percent_correct += utils.topk(torch.abs(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm)), labels, k=1).item()\n",
    "    \n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(voxel)), size=batch_size//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "\n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "\n",
    "    model.eval()\n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            for test_i, (behav, past_behav, future_behav, old_behav) in enumerate(test_dl):  \n",
    "                # all test samples should be loaded per batch such that test_i should never exceed 0\n",
    "                assert len(behav) == num_test\n",
    "                \n",
    "                ## Average same-image repeats ##\n",
    "                if test_image is None:\n",
    "                    voxel = voxels[behav[:,0,5].cpu().long()]\n",
    "                    action = behav[:, 0, target_behaviours].half()\n",
    "\n",
    "                    if seq_len > 1:\n",
    "                        past_behavior = past_behav[:,:(seq_len-1),5].cpu().long()\n",
    "                        past_voxel = voxels[past_behavior]\n",
    "                        if torch.any(past_behavior==-1).item(): # remove invalid voxels (-1 if there is no timepoint available)\n",
    "                            past_voxel[torch.where(past_behavior==-1)[0]] = 0\n",
    "                        past_voxel = torch.Tensor(past_voxel).to(data_type)\n",
    "                \n",
    "                    image = behav[:,0,0].cpu().long()\n",
    "                \n",
    "                    unique_image = torch.unique(image)\n",
    "                    for im in unique_image:\n",
    "                        locs = torch.where(im == image)[0]\n",
    "                        if len(locs)==1:\n",
    "                            locs = locs.repeat(3)\n",
    "                        elif len(locs)==2:\n",
    "                            locs = locs.repeat(2)[:3]\n",
    "                        assert len(locs)==3\n",
    "                        if test_image is None:\n",
    "                            test_image = images[im][None]\n",
    "                            test_voxel = voxel[locs][None]\n",
    "                            test_action = action[locs][None]\n",
    "                            if seq_len > 1:\n",
    "                                test_past_voxel = past_voxel[locs][None]\n",
    "                        else:\n",
    "                            test_image = torch.vstack((test_image, images[im][None]))\n",
    "                            test_voxel = torch.vstack((test_voxel, voxel[locs][None]))\n",
    "                            test_action = torch.vstack((test_action, action[locs][None]))\n",
    "                            if seq_len > 1:\n",
    "                                test_past_voxel = torch.vstack((test_past_voxel, past_voxel[locs][None]))\n",
    "    \n",
    "                # validation sample of 300\n",
    "                val_indices = torch.arange(len(test_voxel))[:test_batch_size]\n",
    "                voxel = test_voxel[val_indices].to(device)\n",
    "                action = test_action[val_indices].to(device)\n",
    "                if seq_len > 1: past_voxel = test_past_voxel[val_indices].to(device)\n",
    "                image = test_image[val_indices].to(device)\n",
    "                assert len(image) == 300\n",
    "                \n",
    "                for rep in range(3):\n",
    "                    voxel_ridge = model.ridge(voxel[:,rep])\n",
    "                    if seq_len > 1:\n",
    "                        past_voxel_ridge = model.ridge(past_voxel[:,rep])\n",
    "                        voxel_ridge = torch.cat((voxel_ridge, past_voxel_ridge), axis=1)\n",
    "                    # clip_voxels0, blurry_image_enc_0 = model.backbone(voxel_ridge)\n",
    "                    clip_voxels0, blurry_image_enc_0 = model.backbone(voxel_ridge, action[:,rep])\n",
    "                    if rep==0:\n",
    "                        clip_voxels = clip_voxels0\n",
    "                        if blurry_recon: blurry_image_enc_ = blurry_image_enc_0\n",
    "                    else:\n",
    "                        clip_voxels += clip_voxels0\n",
    "                        if blurry_recon: blurry_image_enc_ += blurry_image_enc_0\n",
    "                clip_voxels /= 3\n",
    "                if blurry_recon: \n",
    "                    blurry_image_enc_ /= 3\n",
    "                    blurry_image_enc = autoenc.encode(2*utils.resize(image,128)-1).latent_dist.mode() * 0.18215\n",
    "            \n",
    "                clip_target = clip_model.embed_image(image.float())\n",
    "    \n",
    "                clip_voxels_norm = nn.functional.normalize(clip_voxels.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "        \n",
    "                loss_clip = utils.soft_clip_loss(\n",
    "                    clip_voxels_norm,\n",
    "                    clip_target_norm,\n",
    "                    temp=.006)\n",
    "                test_loss_clip_total += loss_clip.item()\n",
    "                loss_clip = loss_clip * clip_scale\n",
    "                loss = loss_clip\n",
    "\n",
    "                if blurry_recon:\n",
    "                    loss_blurry = l1(blurry_image_enc_, blurry_image_enc)\n",
    "                    test_loss_blurry_total += loss_blurry.item()\n",
    "                    loss_blurry *= blur_scale\n",
    "                    loss += loss_blurry\n",
    "    \n",
    "                    # halving the batch size because the decoder is computationally heavy\n",
    "                    blurry_recon_images = (autoenc.decode(blurry_image_enc_[:len(voxel)//2]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    blurry_recon_images = torch.vstack((blurry_recon_images, (autoenc.decode(blurry_image_enc_[len(voxel)//2:]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    pixcorr = utils.pixcorr(image, blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "        \n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_target_norm)).to(clip_voxels_norm.device) \n",
    "                test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_voxels_norm, clip_target_norm), labels, k=1).item()\n",
    "                test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_voxels_norm), labels, k=1).item()\n",
    "\n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "\n",
    "            print(\"---\")\n",
    "            \n",
    "            assert (test_i+1) == 1\n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/lr\": lrs[-1],\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"test/loss_blurry_total\": test_loss_blurry_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                }\n",
    "    \n",
    "            if blurry_recon:    \n",
    "                # transform blurry recon latents to images and plot it\n",
    "                fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                jj=-1\n",
    "                for j in [0,1,2,3]:\n",
    "                    jj+=1\n",
    "                    axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    axes[jj].axis('off')\n",
    "                    jj+=1\n",
    "                    axes[jj].imshow(utils.torch_to_Image((autoenc.decode(blurry_image_enc_[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                    axes[jj].axis('off')\n",
    "                \n",
    "                if wandb_log:\n",
    "                    logs[f\"test/recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                    plt.close()\n",
    "                else:\n",
    "                    plt.show()\n",
    "            \n",
    "            progress_bar.set_postfix(**logs)\n",
    "    \n",
    "            # Save model checkpoint and reconstruct\n",
    "            if epoch % ckpt_interval == 0 and ckpt_saving:\n",
    "                if not utils.is_interactive():\n",
    "                    save_ckpt(f'last')\n",
    "                    \n",
    "            if wandb_log: wandb.log(logs)\n",
    "\n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')\n",
    "if not utils.is_interactive():\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93e87fde-815d-4452-9915-f5f5dacf7c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c668cddf-ec92-4f35-a6a3-46708abda233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a80d5c-5713-405e-94ee-d0119fa05ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
